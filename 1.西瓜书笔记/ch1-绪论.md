# 第一章 绪论

1. **一切都是建立在一个假设**，假设样本空间中全体样本服从一个未知“分布”（distribution）D，我们获得的每个样本都是独立地从这个分布上采样获得的，即“独立同分布”（independent and identically distributed，简称i.i.d）.

2. 机器学习算法在学习过程中**对某种类型假设的偏好，称为“归纳偏好”（inductive bias），或简称为“偏好”**。归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或“价值观”。

3. “奥卡姆剃刀”（Occam's razor）是一种常用的、自然科学研究中最基本的原则，**即“若有多个假设与观察一致，则选择最简单的那个。”**

4. “没有免费的午餐”定理（No Free Lunch Theorem，简称NFL定理）有两个很扯淡的前提要求：
   1. **所有“问题”出现的机会相同、或所有问题同等重要。**
   2. **假设了f的均匀分布。**
> 故而，NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义。且归纳偏好要与问题相匹配。