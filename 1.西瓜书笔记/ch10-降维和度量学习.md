# 第十章 降维和度量学习

1. 懒惰学习（lazy learning）：此类学习技术在训练阶段仅仅是把样本报错起来，训练时间开销为零，待收到测试样本后再进行处理。

2. 急切学习（eager learning）：在训练阶段就对样本进行学习处理

3. 最近邻分类器虽简单，但它的泛化错误不超过贝叶斯最优分类器的错误率的两倍！

4. 在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，**被称为“维数灾难”（curse of dimensionality）**

5. **缓解维数灾难的一个重要途径是降维（dimension reduction），亦称“维数约简”，即通过某种数学变换将原始高维属性空间转变为一个低维“子空间”（subspace）**
![20230613130159](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613130159.png)

6. **若要求原始空间中样本之间的距离在低维空间中得以保持**，如上图所示，即得到“多维缩放”（Multiple Dimensional Scaling，简称MDS）。这是一种经典的降维方法。**要求任意两个样本在新的空间中的欧式距离等于原始空间中的距离**.
![20230613131109](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613131109.png)

7. **主成分分析（Principal Component Analysis，简称PCA），亦称“主分量分析”，是最常用的一种降维方法。**
- **PCA的目标是将原始数据集投影到一个新的低维空间中，同时最大程度地保留数据的方差**。在新的低维空间中，每个维度都被称为主成分，其按照数据的方差递减的顺序排列。第一个主成分捕捉到数据中的最大方差，第二个主成分捕捉到与第一个**主成分正交（无关）的最大方差**，以此类推。

8. 线性降维方法假设从高维空间到低维空间的函数映射是线性的，然而，在不少现实任务中，**可能需要非线性映射才能找到恰当的低维嵌入**。

9. **非线性降维的一种常用方法，是基于核技巧对线性降维方法进行“核化”（kernelized）**

10. 流形学习
    1.  等度量映射
    2.  局部线性嵌入

11. 度量学习
**在机器学习中，对高维数据进行降维的主要目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。**事实上，每个空间对应了在样本属性上定义的一个距离度量，而寻找合适的空间，实质上就是在寻找一个合适的距离度量。**那么，为何不直接尝试“学习”出一个合适的距离度量呢？这就是度量学习（metric learning）的基本动机。**

12. **主成分分析（PCA）是迄今最常用的降维方法，它有许多名字，例如线性代数中的散度矩阵奇异值分解（SVD）**、统计学中的因子分析（factor analysis）等等
