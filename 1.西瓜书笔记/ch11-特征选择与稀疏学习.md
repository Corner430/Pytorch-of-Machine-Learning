# 第十一章 特征选择与稀疏学习

1. 对一个学习任务来说，给定属性集， 其中有些属性可能很关键、很有用，另一些属性则可能没什么用。我们将属性称为“特征”（feature），对当前学习任务有用的属性称为“相关特征”（relevant feature）、没什么用的属性称为“无关特征”（irrelevant feature）.**从给定的特征集合中选择出相关特征子集的过程，称为“特征选择”（feature selection）**

2. 特征选择是一个重要的**数据预处理**（data preprocessing）过程，这么做的原因有二：
   1. 维数灾难
   2. 去除不相关特征往往会降低学习任务的难度

3. 特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为重要信息的缺失而无法获得好的性能。

4. 特征选择中所谓的“无关特征”是指与当前学习任务无关。

5. 有一类特征称为“冗余特征”（redundant feature），它们所包含的信息能从其他特征中推演出来。更确切地说，**若某个冗余特征恰好对应了完成学习任务所需的“中间概念”，则该冗余特征是有益的**

6. 欲从初始的特征集合中选取一个包含了所有重要信息的特征子集，**若没有任何领域知识作为先验假设，那就只好遍历所有可能的子集了**，然而这在计算上却会遭遇**组合爆炸问题**.

7. 特征选择涉及两个关键环节
   1. **子集搜索（subset search）**：如何根据评价结果获取下一个候选特征子集。
      1. 逐渐增加相关特征的策略称为“前向”（forword）搜索
      2. 逐渐减少特征的策略称为“后向”（backward）搜索
      3. 还可将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征（这些特征在后续轮中将确定不会被去除）、同时减少无关特征，这样的策略称为“双向”（bidrectional）搜索
   2. 子集评价（subset evaluation）：如何评价候选特征子集的好坏
      1. 可计算属性子集A的**信息增益**。信息增益Gain(A)越大，意味着特征子集A包含的有助于分类的信息越多。于是，对每个候选特征子集，我们可基于训练数据集D来计算其信息增益，以此作为评价准则。
      2. 更一般的，特征子集A实际上确定了对数据集D的一个划分，每个划分区域对应着A上的一个取值，而样本标记信息Y则对应着对D的**真实划分**，通过估算这两个划分的差异，就能对A进行评价。**与Y对应的划分的差异越小，则说明A越好**。
      3. **信息熵仅是判断这个差异的一种途径，其他能判断两个划分差异的机制都能用于特征子集评价**.
> 将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。

8. **稀疏表示（Sparse Representation）和字典学习（Dictionary Learning）**
- 稀疏表示（Sparse Representation）和字典学习（Dictionary Learning）是一种用于信号处理和机器学习中的技术，旨在通过寻找最佳的**线性**表示来描述数据。

- 稀疏表示是一种表示方法，它假设信号可以由一组基向量的**线性组合**表示，而这组基向量中只有很少一部分起作用。这意味着信号可以用很少的非零系数表示，其他系数为零。这种表示的优势在于能够提取出数据中的关键信息，并且具有较强的鲁棒性。

- 字典学习是稀疏表示的一种方法，它通过学习一个字典（也称为原子集合），其中包含一组基向量，这些基向量可以线性组合来表示数据。字典的目标是使得数据能够以稀疏的方式表示，即用尽可能少的基向量来表示尽可能多的数据。字典学习的过程通常包括以下几个步骤：
  - 初始化字典：选择一组初始的基向量作为字典。
  - 稀疏编码：使用某种优化算法，对数据进行稀疏编码，即寻找使得数据以稀疏方式表示的系数。
  - 字典更新：根据稀疏编码的结果，通过更新字典中的基向量，使其能够更好地表示数据。常见的更新方法包括迭代最小化（iterative minimization）和梯度下降等。
  - 重复步骤2和3，直到达到收敛条件或满足预定的停止准则。

- **字典学习的目标是找到一个最佳的字典，使得数据能够以最少的基向量表示，从而提取出数据的最重要特征并去除冗余信息**。它在信号处理、图像处理、模式识别等领域中具有广泛的应用，例如图像压缩、人脸识别、目标检测等。字典学习的好处在于能够适应不同类型的数据，并从数据中学习到最具代表性的基向量。

9. 压缩感知（Compressed Sensing）
- 压缩感知（Compressed Sensing）是一种基于信号处理和数学理论的新兴技术，**用于从少量的测量数据中恢复原始信号**。传统的信号采样理论要求对信号进行高密度的采样，但压缩感知则允许在采样时以远远低于传统采样要求的采样率进行采样。

- 压缩感知的核心思想是，**对于稀疏或具有某种结构的信号，可以使用远远少于信号维度的线性测量进行采样，并通过优化算法重构原始信号**。稀疏性指的是信号在某个特定域中具有较少的非零系数，或者说信号可以通过较少的基向量进行稀疏表示。

- 压缩感知的算法通常包括以下几个步骤：
  - 采样：使用低采样率的线性测量设备对信号进行采样。采样矩阵可以是随机矩阵或具有特定结构的矩阵。
  - 稀疏表示：使用适当的稀疏变换或字典，将采样到的信号表示为稀疏系数的线性组合。
  - 重构：通过优化算法，寻找最优的稀疏系数，使其与测量数据保持一致。常用的优化算法包括基于凸优化的方法，如迭代阈值算法（Iterative Thresholding）、最小二乘法（Least Squares）等。

- 压缩感知的关键概念是信号的稀疏表示和稀疏重构算法。通过采用低采样率和合适的稀疏表示方法，压缩感知能够从少量的测量数据中重构出高质量的信号。这种技术在信号处理、图像处理、通信等领域具有广泛的应用，可以实现低成本、高效率的信号采样和传输。
-------------------------------------------------

## 常见的特征选择方法
常见的特征选择方法大致可分为三类：**过滤式（filter）、包裹式（wrapper）和嵌入式（embedding）**

1. **过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关**

2. 包裹式选择
   1. 与过滤式特征选择不考虑后续学习器不同，包裹式特征选择**直接把最终将要使用的学习器的性能作为特征子集的评价准则**.换言之，包裹式特征选择的目的**就是为给定学习器选择最有利于其性能、“量身定做”的特征子集**
   2. 由于包裹式特征选择方法直接针对给定学习器进行优化，**因此**从最终学习器性能来看，**包裹式特征选择比过滤式特征选择更好**，但另一方面，由于在特征选择过程中需多次训练学习器，因此**包裹式特征选择的计算开销通常比过滤式特征选择大得多**.

3. 嵌入式选择与$L_1$正则化
在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别；**与此不同，嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。**

![20230613184850](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613184850.png)

> $L_1$范数和$L_1$范数正则化都有助于降低过拟合风险，但前者还会带来一个额外的好处：它比后者更易于获得“稀疏”（sparse）解，即它求得的$\omega$会有很少的非零向量

![20230613185347](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613185347.png)

> 注意到$\omega$取得稀疏解意味着初始的d个特征中仅有对应着的$\omega$非零分量的特征才会出现在最终模型中，于是求解$L_1$范数正则化的结果是得到了仅采用一部分初始特征的模型；换言之，基于$L_1$正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成.
