# 第十六章 强化学习

1. 强化学习（reinforcement learning）
  - 强化学习（Reinforcement Learning，RL）是机器学习的一个分支，**用于解决智能体（Agent）在与环境进行交互的过程中，通过试错和学习来获得最优行为策略的问题**。强化学习是一种通过奖励信号来引导学习的方法，智能体通过与环境的交互，通过尝试不同的行动并接收环境的反馈来学习如何在给定的环境中做出最佳的决策。
  - 强化学习涉及以下几个核心元素：
    1. 环境（Environment）：强化学习中智能体所处的外部环境，它包含智能体**可以观察到的状态和智能体可以采取的动作**。
    2. 智能体（Agent）：强化学习的学习主体，通过观察环境的状态，选择合适的动作以最大化累积奖励。
    3. 状态（State）：环境的观察值，用于描述环境的特定情况。
    4. 动作（Action）：智能体在给定状态下可以执行的操作或决策。
    5. 奖励（Reward）：环境在智能体执行动作后**提供的反馈信号，用于指导智能体的学习过程。奖励可以是正向的（鼓励行为）或负向的（惩罚行为）**。
  - 强化学习的目标是通过学习最优的策略，使智能体在环境中**获得最大的累积奖励。学习过程通常基于价值函数（Value Function）或策略（Policy）的优化**。价值函数用于评估在给定状态下采取特定动作的**长期回报，而策略定义了智能体在给定状态下应该采取的动作**。
  - 强化学习算法可以基于**模型（Model-Based）或无模型（Model-Free）进行分类**。模型基础的方法通过学习环境的模型（包括状态转移和奖励函数）来做出决策。无模型的方法则直接从与环境的交互中学习，通过尝试和错误来更新价值函数或策略。
  - 强化学习在许多领域有广泛应用，包括机器人控制、自动驾驶、游戏玩法、资源管理和金融交易等。著名的强化学习算法包括Q-learning、SARSA、深度强化学习（Deep Reinforcement Learning）和策略梯度（Policy Gradient）等。
  - **总而言之，强化学习是一种通过试错和学习来获得最优行为策略的机器学习方法。它通过智能体与环境的交互和奖励信号来引导学习，以实现在给定环境下最大化累积奖励的目标**。

2. 马尔可夫决策过程（Markov Decision Process，MDP）
- **马尔可夫决策过程（Markov Decision Process，MDP）是一种用于建模序贯决策问题的数学框架**。它基于马尔可夫性质，**描述了在随机环境中，智能体通过采取不同的动作来达到目标的决策过程**。

- MDP由五个核心要素组成：
  1. 状态（State）：表示智能体所处的环境状态。**在每个时间步，智能体通过观察当前状态来做出决策**。
  2. 动作（Action）：表示智能体可以采取的行动或决策。**在给定状态下，智能体可以选择执行的动作集合**。
  3. 转移概率（Transition Probability）：**表示从一个状态到另一个状态的转移概率**。给定当前状态和采取的动作，转移概率描述了下一个状态的**可能性分布**。
  4. 奖励（Reward）：表示智能体在状态转移过程中获得的**即时反馈**。奖励可以是正向的（鼓励行为）或负向的（惩罚行为）。
  5. 折扣因子（Discount Factor）：**用于权衡当前奖励和未来奖励的重要性。折扣因子决定了未来奖励对智能体决策的相对重要性**。

- 基于这些要素，MDP的**目标是找到一个最优策略（Policy），使智能体在给定状态下能够选择最佳的动作以最大化累积奖励**。最优策略可以通过价值函数（Value Function）来衡量，价值函数表示从给定状态开始，智能体在执行最优策略下可以获得的长期回报。

- MDP的求解可以使用**动态规划方法，其中常用的算法包括值迭代（Value Iteration）和策略迭代（Policy Iteration）**。值迭代通过迭代更新价值函数来逐步逼近最优策略，而策略迭代则通过反复改进策略和更新价值函数来获得最优策略。**策略迭代算法在每次改进策略后都需重新进行策略评估，这通常比较耗时。策略改进与值函数的改进是一致的，因此可将策略改进视为值函数的改善**

- 马尔可夫决策过程在强化学习中具有重要的地位，被广泛应用于智能体在复杂环境中进行决策和规划的问题，如机器人控制、自动驾驶和资源管理等领域。

3. 总之，在环境中状态的转移、奖赏的返回是不受机器控制的，**机器只能通过选择要执行的动作来影响环境，也只能通过观察转移后的状态和返回的奖赏来感知环境**

4. 机器要做的是通过在环境中不断尝试而学得一个“策略”（policy）$\pi$。策略有两种表示办法：一种是将策略表示为函数$\pi : X \rightarrow A$，确定性策略常用这种表示；另一种是概率表示$\pi : X x A \rightarrow \mathbb{R}$，随机性策略常用这种表示。

5. **浅谈强化学习与监督学习的差别**
![20230614150202](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614150202.png)

## 16.1 K-摇臂赌博机（K-Armed Bandit），亦称“K-摇臂老虎机”
- 探索与利用
  - 强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：**最大化单步奖赏，即仅考虑一步操作。欲最大化单步奖赏需考虑两个方面**：
    - 每个动作带来的奖赏
    - 要执行奖赏最大的动作
  若每个动作对应的奖赏是一个确定值，那么尝试一遍所有的动作便能找出奖赏最大的动作。**然而，更一般的情形是，一个动作的奖赏值是来自于一个概率分布，仅通过一次尝试并不能确切地获得平均奖赏值**.

- **这样的单步强化学习任务对应了一个理论模型，即“K-摇臂赌博机”（K-armed bandit）**。K-摇臂赌博机有K个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。

- **若仅为获知每个摇臂的期望奖赏，则可采用仅探索”（exploration-only）**法：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。**若仅为执行奖赏最大的动作，则可采用“仅利用”（exploitation-only）**法：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。
> 显然，“仅探索”法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；“仅利用”法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。

- 事实上，**“探索”(即估计摇臂的优劣)和“利用”(即选择当前最优摇臂)这两者是矛盾的**，因为尝试次数(即总投币数)有限，加强了一方则会自然削弱另一方，**这就是强化学习所面临的“探索利用窘境”(Exploration-Exploitation dilemma)**。显然，**欲累积奖赏最大，则必须在探索与利用之间达成较好的折中.**

6. K-摇臂赌博机（K-Armed Bandit）是强化学习中的一个经典问题。**它模拟了在面临多个选择（摇臂）时，如何在有限的资源和不确定性环境下进行决策的情境**。
- K-摇臂赌博机的设定如下：
  1. 摇臂（Arms）：有K个摇臂可供选择，每个摇臂**都有一个固定的概率分布**，表示在选择该摇臂时获得奖励的概率分布。
  2. 奖励（Reward）：在每次选择摇臂后，根据所选择摇臂的概率分布随机产生一个奖励，奖励可以是实数值或离散值。
  3. 动作选择（Action Selection）：在每个时间步，智能体需要选择一个摇臂进行操作。选择哪个摇臂是智能体的决策。
  4. 目标：智能体的目标是通过与摇臂的交互累积获得最大的奖励。

- **在K-摇臂赌博机问题中，智能体需要在探索（Exploration）和利用（Exploitation）之间做出权衡。探索是指尝试选择未知的摇臂以了解它们的概率分布和奖励情况，以便获取更准确的信息。利用是指选择已知的高奖励摇臂，以最大化当前获得的奖励**。

- 解决K-摇臂赌博机问题的关键是找到一个合适的策略，即在探索和利用之间找到平衡点。常用的策略包括：
  1. ε-贪心策略（ε-Greedy）：以1-ε的概率选择当前奖励最高的摇臂，以ε的概率选择随机摇臂，以促进探索。
  2. 上限置信界（Upper Confidence Bound，UCB）：根据奖励的置信界，选择具有最高上界的摇臂，以平衡探索和利用。
  3. 梯度赌博算法（Gradient Bandit Algorithm）：使用梯度信息来调整选择每个摇臂的概率，以最大化奖励的期望。

- 这些策略都有不同的权衡，探索能力强的策略可能会获得更多信息，但可能在短期内获得较低的奖励；而利用能力强的策略可能会在短期内获得较高的奖励，但可能会错过更高奖励的摇臂。

- K-摇臂赌博机问题作为一个简单但具有挑战性的强化学习问题，可以帮助我们理解和研究在有限资源和不确定环境下的决策过程，并为更复杂的强化学习问题提供基础。

7. $\epsilon$-贪心法基于一个概率来对探索和利用进行折中：每次尝试时，以$\epsilon$的概率进行探索，即以均匀概率随机选取一个摇臂；以$1 - \epsilon$的概率进行利用，即选择当前平均奖赏最高的摇臂(若有多个，则随机选取一个).

> 若摇臂奖赏的不确定性较大，例如概率分布较宽时，则需更多的探索，此时需要较大的$\epsilon$值；若摇臂的不确定性较小，例如概率分布较集中时，则少量的尝试就能很好地近似真实奖赏，此时需要的$\epsilon$较小。通常令$\epsilon$取一个较小的常数，如0.1或0.01。**然而，若尝试次数非常大，那么在一段时间后，摇臂的奖赏都能很好地近似出来，不再需要探索，这种情形下可让$\epsilon$随着尝试次数的增加而逐渐减小**，例如令$\epsilon = 1/\sqrt{t}$.

8. Softmax
- Softmax算法基于**当前已知的摇臂平均奖赏**来对探索和利用进行折中。**若各摇臂的平均奖赏相当，则选取各摇臂的概率也相当；若某些摇臂的平均奖赏明显高于其他摇臂,则它们被选取的概率也明显更高.**
![20230614155654](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614155654.png)

9. 对于离散状态空间、离散动作空间上的**多步强化学习任务，一种直接的办法是将每个状态上动作的选择看作一个K-摇臂赌博机问题，用强化学习任务的累积奖赏来代替K-摇臂赌博机算法中的奖赏函数**，即可将赌博机算法用于每个状态：对每个状态分别记录各动作的尝试次数、当前平均累积奖赏等信息，基于赌博机算法选择要尝试的动作。**然而这样的做法有很多局限，因为它没有考虑强化学习任务马尔可夫决策过程的结构**。若能有效考虑马尔可夫决策过程的特性，则可有更聪明的办法.

10. **考虑多步强化学习任务，暂且先假定任务对应的马尔可夫决策过程四元组E = (X，A，P，R)均为已知，这样的情形称为“模型已知”**，即机器已对环境进行了建模，能在机器内部模拟出与环境相同或近似的状况。**在已知模型的环境中学习称为“有模型学习”(model-based learning)**.

11. 在现实的强化学习任务中，环境的转移概率、奖赏函数往往很难得知，甚至很难知道环境中一共有多少状态。**若学习算法不依赖于环境建模，则称为“免模型学习”(model-free learning)，亦称“无模型学习”，这比有模型学习要困难得多**.

12. **在免模型情形下，策略迭代算法首先遇到的问题是策略无法评估，这是由于模型未知而导致无法做全概率展开**。此时，只能通过在环境中执行选择的动作，来观察转移的状态和得到的奖赏。受K摇臂赌博机的启发，**一种直接的策略评估替代方法是多次“采样”，然后求取平均累积奖赏来作为期望累积奖赏的近似，这称为蒙特卡罗强化学习**。由于采样必须为有限次数，因此该方法更适合于使用T步累积奖赏的强化学习任务.

13. **蒙特卡罗强化学习（Monte Carlo Reinforcement Learning）是一种基于样本采样的强化学习方法。它通过与环境的交互来获取经验数据，并利用这些数据进行策略评估和策略改进**。
- 蒙特卡罗强化学习的核心思想是基于回报样本的累积来评估策略的好坏。**具体而言，它通过执行一系列完整的回合（Episode）来采样轨迹数据**。在每个回合中，智能体与环境进行交互，根据当前的策略选择动作，并观察环境的反馈（奖励和下一个状态）。通过多次回合的交互，蒙特卡罗方法可以得到一组回合样本。

- 利用这些回合样本，蒙特卡罗强化学习主要包含两个步骤：**策略评估和策略改进**。
  1. 策略评估（Policy Evaluation）：在策略评估中，通过对回合样本的回报进行累积，估计每个状态的值函数或状态动作对值函数。一种常用的方法是使用蒙特卡罗预测（Monte Carlo Prediction），根据回合样本计算状态值函数或状态动作对值函数的平均回报。
  2. 策略改进（Policy Improvement）：在策略改进中，基于估计的值函数，可以对策略进行改进。常用的方法是使用ε-贪心策略（ε-Greedy Policy），以一定概率选择当前估计值最高的动作，以提高利用能力；同时，以一定概率选择随机动作，以增加探索能力。策略改进可以根据当前值函数的估计，生成新的策略。

- 通过反复进行策略评估和策略改进，蒙特卡罗强化学习逐渐优化策略，并逐步收敛到最优策略。

- **蒙特卡罗强化学习的优点是它不需要对环境的转移概率进行假设，只利用交互的样本数据进行学习。然而，由于需要采样大量的回合样本，蒙特卡罗方法通常具有较高的计算成本和样本效率。此外，它在处理连续状态和动作空间的问题上存在一定的挑战**。

- 总之，蒙特卡罗强化学习是一种基于样本采样的强化学习方法，通过采样回合样本来评估和改进策略。它是强化学习中重要的方法之一，为解决复杂的强化学习问题提供了一种有效的学习框架。

14. **时序差分学习（Temporal Difference Learning）是一种强化学习方法，它结合了动态规划和蒙特卡罗方法的优点。时序差分学习用于估计值函数或值函数的更新，通过观察到的即时奖励和预测的未来奖励之间的差异来进行学习**。

- 在时序差分学习中，智能体通过与环境的交互来获取经验数据。在每个时间步，智能体观察当前状态，选择动作并执行，然后观察环境的反馈（奖励）和下一个状态。智能体使用这些信息来**更新值函数的估计**。

- 时序差分学习使用**贝尔曼方程（Bellman equation）作为基本的更新规则**。贝尔曼方程描述了值函数的递归关系，将当前状态的值与下一个状态的值之间建立了联系。

- 具体而言，**时序差分学习中最常用的方法是Q-learning**。Q-learning使用一个Q值函数（动作值函数）来估计在给定状态下执行每个动作的价值。在每个时间步，Q-learning通过以下更新规则来更新Q值函数的估计：

$$Q(s, a) = Q(s, a) + \alpha [r + \gamma max_a' Q(s', a') - Q(s, a)]$$

> 其中，Q(s, a)表示在状态s下执行动作a的Q值，r表示即时奖励，s'表示下一个状态，a'表示下一个状态下的最优动作，$\alpha$表示学习率（用于控制更新幅度），$\gamma$表示折扣因子（用于平衡即时奖励和未来奖励的重要性）。

- Q-learning通过迭代地更新Q值函数，逐渐收敛到最优的Q值函数。一旦获得最优的Q值函数，智能体可以根据该函数选择具有最高Q值的动作作为策略。

- **时序差分学习的优点是它可以在每个时间步进行更新，不需要等待整个回合的结束，因此具有较高的样本效率。它还能够处理连续状态和动作空间，并且能够在未知环境中进行在线学习**。

- 总之，时序差分学习是一种强化学习方法，通过观察到的即时奖励和预测的未来奖励之间的差异来更新值函数的估计。Q-learning是时序差分学习的一种常用方法，通过迭代更新Q值函数来实现值函数的学习和策略的改进。

15. **值函数近似是一种在强化学习中用于估计值函数的方法。它的主要思想是通过函数逼近器（如线性函数、神经网络等）来近似值函数，而不是显式地存储和计算所有可能状态的值**。
- 在强化学习中，值函数用于估计每个状态或状态动作对的价值，表示在当前状态下采取行动的预期回报。然而，对于大规模状态空间或连续状态空间的问题，显式存储和计算值函数是不可行的。值函数近似通过使用函数逼近器来解决这个问题，将状态空间映射到一个参数化的函数空间中。

- 值函数近似的基本步骤如下：
  1. 选择一个函数逼近器：值函数近似使用函数逼近器来估计值函数。常见的函数逼近器包括线性函数、多项式函数、神经网络等。函数逼近器的选择取决于具体问题的性质和复杂度。
  2. 定义特征表示：为了进行值函数近似，需要将状态或状态动作对映射到函数逼近器所能接受的输入格式。通常，需要定义一组特征表示来描述状态或状态动作对的属性。这些特征可以是原始状态的函数变换、经验数据的统计特征等。
  3. 参数估计：使用强化学习算法，根据采集的样本数据，对函数逼近器的参数进行估计。常见的算法包括梯度下降、随机梯度下降、Q-learning等。通过与环境的交互，根据奖励信号和下一个状态的值函数估计，更新函数逼近器的参数。
  4. 策略改进：根据近似的值函数，可以使用贪心策略或其他策略选择方法来改进智能体的策略。根据值函数近似的结果，智能体可以选择具有最高估计值的动作作为策略。

- 值函数近似的优点是它能够处理大规模状态空间或连续状态空间的问题，节省了存储和计算的开销。然而，它也面临着一些挑战，如函数逼近器的选择、特征表示的设计和参数估计的稳定性等。

- **总结而言，值函数近似是一种通过使用函数逼近器来估计值函数的方法。它通过将状态映射到函数空间中，解决了大规模状态空间或连续状态空间问题中的存储和计算困难**。

16. **模仿学习（Imitation Learning）是一种强化学习的方法，旨在通过观察和模仿专家（或者已经解决任务的人）的行为来学习任务的策略**。模仿学习的目标是从专家的示范中学习到一个能够执行类似行为的策略，而无需进行探索和试错。
- 在模仿学习中，通常存在两种主要方法：**直接模仿学习和逆强化学习**。
  - 直接模仿学习（Direct Imitation Learning）：
    - 直接模仿学习是一种简单直接的模仿方法，它通过收集专家演示的样本数据，然后直接使用这些数据来训练一个模型，例如神经网络。模型会尽量复制专家示范的行为。在训练过程中，模型学习将输入状态映射到输出动作的映射关系，以实现与专家类似的策略。**直接模仿学习可以使用监督学习的方法来进行训练，其中输入是状态，输出是动作**。
    - **直接模仿学习的优点是简单直接，容易实现。然而，它也面临一些挑战。例如，直接模仿学习可能会受到专家演示中的噪声和偏差的影响，导致模型学到错误的策略。此外，直接模仿学习无法解决探索问题，因为它仅仅是复制专家的行为而不具备主动探索新策略的能力**。
  - 逆强化学习（Inverse Reinforcement Learning）：
    - **逆强化学习是一种基于观察者行为来推断其背后的奖励函数的方法**。在逆强化学习中，智能体试图从专家示范中学习到任务的奖励函数，然后根据这个奖励函数来执行任务。**逆强化学习的目标是理解专家示范的动机和目标，而不仅仅是模仿其行为**。
    - 逆强化学习的基本思想是通过比较专家示范的行为与候选奖励函数生成的行为来推断奖励函数。通过迭代的过程，逆强化学习算法可以找到与专家示范最一致的奖励函数。**一旦获得了奖励函数，智能体可以使用强化学习方法来学习最优策略**。
    - **逆强化学习的优点是它能够从专家示范中推断出更深层次的意图和目标，使得智能体能够更加灵活地适应环境和任务变化。然而，逆强化学习也存在一些挑战，例如推断奖励函数的不确定性、计算复杂度和过拟合等问题**。

总结而言，**模仿学习是一种通过观察和模仿专家的行为来学习任务策略的方法。直接模仿学习直接使用专家示范来训练模型，而逆强化学习通过推断奖励函数来学习专家的目标和动机**。这些方法在实际应用中具有各自的优缺点，可以根据具体问题选择合适的方法。