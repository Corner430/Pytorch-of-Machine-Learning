# 第四章 决策树

1. 决策树的递归生成
![20230608163731](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230608163731.png)

2. 决策树学习的关键在于如何选择最优划分属性。一般而言，锁着划分过程不断进行，**我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即节点的“纯度（purity）越来越高**

3. **“信息熵”(information entropy)**是度量样本集合纯度最常用的一种指标。**Ent(D)的值越小，则D的纯度越高**.

4. **信息增益（information gain）**
![20230608164358](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230608164358.png)

5. **信息增益准则对可取值数目较多的属性有所偏好，**为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法不直接使用信息增益，而是使用**“增益率”（gain ratio）** 来选择最优划分属性。但是**增益率准则对可取值数目较少的属性有所偏好**，因此便产生了一种启发式的方法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

6. **数据集D的纯度也可用基尼值来度量。**直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。**因此Gini(D)越小，则数据集D的纯度越高。** 著名的CART决策树就是使用“基尼指数（Gini index）来选择划分属性。

7. **决策树的剪枝分为“预剪枝”（prepruning）和“后剪枝”（postpruning）**

8. 剪枝：由**验证集精度**是否提升来决定本次是否剪枝

9. 后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情形下，**后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但是时间开销要比着预剪枝多很多。**

10. 决策树也可对连续属性进行处理，此时要用到**连续属性离散化**。<font color="#dd0000">需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性</font><br />

11. 样本属性值缺失也是决策树要考虑的问题

12. **多变量决策树**
若我们把每个属性视为坐标空间中的一个坐标轴,则d个属性描述的样本就对应了d维空间中的一个数据点;对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界.决策树所形成的分类边界有一个明显的特点:轴平行(axis- parllel),即它的分类边界由若干个与坐标轴平行的分段组成.这样的分类边界使得学习结果有较好的可解释性，但在某些时候却会稍显乏力.
![20230608171237](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230608171237.png)
![20230608171250](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230608171250.png)

13. 决策树学习算法最著名的代表是ID3、C4.5、CART.

14. 比较花里胡哨的决策树
![20230608171531](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230608171531.png)