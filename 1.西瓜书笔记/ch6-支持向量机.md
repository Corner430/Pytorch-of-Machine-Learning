# 第六章 支持向量机

1. **正中间的超平面**对于训练集的局限性和噪声容忍性最好，最鲁棒。
2. **使用拉格朗日乘子法将原问题转化为对偶问题，注意每个拉格朗日乘子都要大于等于0**
3. 训练完成后，大部分的训练样本都不需要保留，**最终模型仅与支持向量有关**
4. SMO算法：先固定$\alpha_i$之外的所有参数，然后求上的极值$\alpha_i$
![20230608230636](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230608230636.png)

5. **如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分**。核函数做的事就等同于做这个维度映射。通过核函数进行非显式的计算特征空间的内积，就叫做“核技巧”
6. 什么样的函数可以作为核函数？
![20230608231139](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230608231139.png)
7. 常用核函数
![20230608231238](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230608231238.png)
8. 事实上，并非所有的训练样本在样本空间或特征空间中都是线性可分的。就算找到了一个核函数使得训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果是不是由于过拟合所造成的。**缓解该问题的一个办法是允许支持向量机在一些样本上出错，为此要引入“软间隔”的概念**
9. 软间隔中每个样本都有一个对应的松弛变量，用以表征该样本不满足约束的程度。
10. **多核学习（multiple kernel learning）是使用多个核函数并通过学习获得其优化凸组合作为最终的核函数，这实际上是在借助集成学习机制**
11. **SVM的确与神经网络有密切联系**：若将隐层神经元数设置为训练样本数，且每个训练样本对应一个神经元中心，则以高斯径向基函数为激活函数的RBF网络恰与高斯核SVM的预测函数相同。
