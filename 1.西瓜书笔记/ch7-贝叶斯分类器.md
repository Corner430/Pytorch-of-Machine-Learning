# 第七章 贝叶斯分类器

1. **对于分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判所示来选择最优的类别标记**

2. 贝叶斯判定准则（Bayes decision rule）：**为最小化总体风险，只需在每个样本上选择那个能使条件风险R(c|x)最小的类别标记**

3. 为便于讨论，假设所有属性均为离散型。对连续属性，可将概率质量函数P(·)换成概率密度函数p(·)
![20230611161623](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230611161623.png)
![20230611161652](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230611161652.png)

4. [贝叶斯估计、最大似然估计、最大后验概率估计](https://corner430.github.io/2023/04/03/Maximum-Likelihood-Estimation-Bayesian-Estimation-and-Maximum-A-Posteriori-Estimation/)
![20230611163211](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230611163211.png)

5. **基于有限训练样本直接估计联合概率，在计算上将会遭遇组合爆炸问题，在数据上将会遭遇样本稀疏问题；属性数越多，问题越严重**

6. 不难发现，基于贝叶斯公式来估计后验概率P(c|x)的**主要困难**在于：类条件概率P(x|c)是所有属性上的联合概率，难以从有限的训练样本直接估计而得。为避开这个障碍，朴素贝叶斯分类器(naive Bayes classifier)采**用了“属性条件独立性假设”(attribute conditional independence assumption)**：对已知类别，假设所有属性相互独立。换言之，假设每个属性独立地对分类结果生影响。

7. 若某个属性值在**训练集中没有与某个类同时出现过**，则直接进行概率估计，之后进行判别将出现问题。为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，**在估计概率值时通常要进行“平滑”（smoothing），常用“拉普拉斯修正”（Laplacian correction）**

8. **显然拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零的问题**，并且在训练集变大时，修正过程所引入的先验（prior）的影响也会逐渐变得可忽略，使得估值渐趋向于实际概率值。

9. 拉普拉斯修正实际上**假设了属性值与类别均匀分布**，这是在朴素贝叶斯学习过程中额外引入了关于数据的先验。

10. 朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立。于是，人们尝试对属性条件独立性假设进行一定程度的放松，**由此产生了一类为“半朴素贝叶斯分类器”（semi-naive Bayes classifiers）的学习方法**

- 半朴素贝叶斯分类器的基本想法是**适当考虑一部分属性间的相互依赖信息**，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。 
  - 独依赖估计（One-Dependent Estimator，简称ODE）是半朴素贝叶斯分类器最常用的一种策略。顾名思义，**所谓“独依赖”就是假设每个属性在类别之外最多仅依赖于一个其他属性**。
![20230612162450](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612162450.png)
![20230612162514](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612162514.png)
    - TAN（Tree Augmented naive Bayes）
![20230612162642](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612162642.png)
    - AODE（Averaged One-Dependent Estimator）是一种基于集成学习机制、更为强大的独依赖分类器。与SPODE通过模型选择确定超父属性不同、AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够数据支撑的SPODE集成起来作为最终结果。
  - 贝叶斯网（Bayesian network）亦称“信念网”（belief network），它借助有向无环图（Directed Acyclic Graph，简称DAG）来刻画属性之间的依赖关系，并使用条件概率表（Conditional Probability Table，简称CRT）来描述属性的联合概率分布。
  - <font color="#dd0000">贝叶斯结构有效地表达了属性间的条件独立性</font><br />
![20230612163728](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612163728.png)
![20230612163808](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612163808.png)

11. **若网络结构已知，即属性间的依赖关系已知**，则贝叶斯网的学习过程相对简单，只需通过对训练样本“计数”，估计出每个节点的条件概率表即可。但在现实应用中我们往往并不知晓网格结构，**于是贝叶斯网学习的首要任务就是根据训练数据集来找出结构最“恰当”的贝叶斯网。** **“评分搜索”**是求解这一问题的常用办法。

12. EM算法（Expectation Maximization Algorithm）是一种迭代优化算法，**用于解决在概率统计中的一类问题，特别是在存在隐变量（或未观测变量）的概率模型中的参数估计问题。**
EM算法的核心思想是通过**迭代地进行两个步骤：E步骤（Expectation Step）和M步骤（Maximization Step），来逐步优化参数估计。**
- 以下是EM算法的基本步骤：
  1. **初始化参数**: 首先需要对模型的参数进行初始化。
  2. **E步骤（Expectation Step）**: 在E步骤中，根据当前的参数估计，计算隐变量的后验概率分布。这里的隐变量指的是未观测到的变量。通过计算隐变量的后验概率，可以得到对于每个样本，它属于每个隐变量取值的概率。
  3. **M步骤（Maximization Step）**: 在M步骤中，利用E步骤中计算得到的隐变量后验概率分布，对参数进行更新估计。这个步骤的目标是最大化观测数据的对数似然函数，即通过优化参数来使模型能够更好地拟合观测数据。
  4. **迭代更新**: 重复执行E步骤和M步骤，直到算法收敛或达到预设的停止条件。通常，通过计算参数的变化量来判断算法是否收敛。

EM算法的**优点**是可以用于估计存在隐变量的概率模型的参数，即使在隐变量存在的情况下，也可以对参数进行有效估计。但它也有一些**限制**，例如容易陷入局部最优解、收敛速度可能较慢等。
EM算法在许多领域都有应用，比如混合高斯模型的参数估计、隐马尔可夫模型的参数学习等。它为解决概率统计中的一类问题提供了一种有效的迭代优化方法。
事实上，隐变量估计问题也可通过梯度下降等优化算法求解，但由于求和的项数将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而**EM算法则可看作一种非梯度优化方法。**
