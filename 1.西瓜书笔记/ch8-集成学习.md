# 第八章 集成学习

1. **集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统（multi-classifier system）**、基于学员会的学习（committee-base learning）等。

2. **集成的方式分为“同质”（homogeneous）和“异质”（heterogenous）**
![20230612193309](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612193309.png)

3. 要获得好的集成，个体学习器**应“好而不同”**，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”（diversity），即学习器间具有差异。

4. 假设基分类器的错误率相互独立，则由Hoeffding不等式可知，集成的错误率为：
![20230612193728](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612193728.png)
> 但是，错误率相互独立不可能成立，原因在于个体学习器是为解决同一问题训练出来的。准确性和多样性本就存在冲突。也就是说，如何产生并结合“好而不同”的个体学习器，恰是集成学习研究的核心。

5. 根据个体学习器的生成方式，**目前的集成学习方法大致可分为两大类**，即个体学习器间存在强依赖关系、必须**串行**生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的**并行**化方法；前者的代表是Boosting，后者的代表是Bagging和“随机森林”(Random Forest).

6. **Boosting是一族可将弱学习器提升为强学习器的算法**。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，**再根据基学习器的表现对训练样本分布进行调整**，使得先前学习器做错的训练样本在后续受到更多关注,然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行**加权结合**。Boosting族算法最著名的代表是AdaBoost。

7. 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。
![20230612201702](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612201702.png)

8. 欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然“独立”在现实任务中无法做到,但**可以设法使基学习器尽可能具有较大的差异**。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若千个不同的子集，再从每个数据子集中训练出一个基学习器。这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。然而，为获得好的集成，我们同时还希望个体学习器不能太差。如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。为解决这个问题，我们可考虑**使用相互有交叠的采样子集**.

9. Bagging是并行式集成学习方法最著名的代表。它基于**自助采样法**。在对预测输出进行结合时，**Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法**。

10. 值得一提的是，**自助采样过程还给Bagging带来了另一个优点**：由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用作验证集来对泛化性能进行“包外估计”（out-of-bag estimate）

11. 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。

12. 随机森林（Random Forest，简称RF）是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中**引入了随机属性选择**。

13. 随机森林中基学习器的多样性不仅来自**样本扰动，还来自属性扰动**，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。

14. 随机森林简单、容易实现、计算开销小。值得一提的是，**随机森林的训练效率常优于Bagging**，因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，在选择划分属时要对结点的所有属性进行考察，而随机森林使用的“随机型”决策树则只需考察一个属性子集。

15. 学习器结合可能会带来的好处。
![20230612211615](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612211615.png)

16. 学习器的结合策略
    1. 平均法（averaging）：对**数值型输出** $h_i(x) \in \mathbb{R}$，最常见的结合策略是使用平均法。（平均法又分为简单平均法（simple averaging）和加权平均法（weighted averaging））
    2. 投票法：绝对多数投票法（majority voting）、相对多数投票法（plurality voting）和加权投票法（weighted voting）
> **标准的绝对多数投票法提供了“拒绝预测”选项**，这在可靠性要求较高的学习任务中是一个很好的机制。但若学习任务要求必须提供预测结果，则绝对多数投票法将退化为相对多数投票法。因此，在不允许拒绝预测的任务中，绝对多数、相对多数投票法统称为“多数投票法”

    3. 学习法：当训练数据很多时，一种更为强大的组合策略是使用“学习法”，**即通过另一个学习器来进行结合**。

17. **多样性度量（diversity measure）是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度**

18. **多样性增强**
    1.  **数据样本扰动**：数据样本扰动通常是基于采样法。**此类做法简单高效，使用最广。然而**，有一些基学习器对数据样本的扰动不敏感，例如线性学习器、支持向量机、朴素贝叶斯、k近邻学习器等，这样的基学习器称为稳定基学习器（stable base learner），对此类基学习器进行集成往往需使用输入属性扰动等其他机制。
    2.  **输入属性扰动**
    3.  **输出表示扰动**：此类做法的基本思路是对输出表示进行操纵以增强多样性
    4. <font color="#dd0000">算法参数扰动</font><br />：基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初始连接权值等，**通过随机设置不同的参数，往往可产生差别较大的个体学习器，例如“负相关法”（Negative Correlation）显式地通过正则化项来强制个体神经网络使用不同的参数**
      - **对参数较少的算法，可通过将其学习过程中某些环节用其他类似方式代替**，从而达到扰动的目的，例如可将决策树使用的属性选择机制替换成其他的属性选择机制。
      - **值得指出的是，使用单一学习器时通常需使用交叉验证等方法来确定参数值**，这事实上已使用了不同参数训练出多个学习器，只不过最终仅选择其中一个学习器进行使用，而集成学习则相当于把这些学习器都利用起来；由此也可看出，**集成学习技术的实际计算开销并不比使用单一学习器大很多**。
> 不同的多样性增强机制可同时使用。

19. **弱学习等价于强学习**

20. **在集成产生之后再试图通过去除一些个体学习器来获得较小的集成，称为集成修剪（ensemble pruning）**

21. 由于集成包含多个学习器，即便个体学习器有较好的可解释性，集成仍是黑箱模型。
