# 第十三章 半监督学习
1. 主动学习（Active Learning）
  - 主动学习（Active Learning）是一种机器学习的方法，旨在提高训练模型的效率和准确性。在传统的机器学习中，通常需要大量的标记数据来训练模型，然后使用该模型进行预测。但是，标记数据的收集和准备往往是耗时和昂贵的。**主动学习的目标是通过选择最有价值的样本来减少标记数据的需求。**
  - 主动学习的**基本思想是，在训练模型的过程中，不仅仅是被动地使用已有的标记数据，而是主动地选择一些未标记的样本**，然后请人类专家或模型本身来标记这些样本。**选择哪些样本进行标记的过程是主动学习的关键。**
  - **主动学习通常使用一种叫做"不确定性采样"（Uncertainty Sampling）的方法来选择要标记的样本**。这种方法**会在模型对未标记样本进行预测时，关注那些模型预测结果最为不确定的样本**。例如，在二分类问题中，一个样本可能距离分类决策边界很近，模型很难确定其分类，那么这个样本就是一个不确定的样本，可以选择标记它来改善模型的预测能力。
  - 另一种常用的主动学习方法是**基于模型不确定性的抽样方法**，例如使用不确定性估计（Uncertainty Estimation）的方法，如基于不确定性的置信度或熵等指标来选择样本。**这些方法通常利用模型在样本空间的不确定性来衡量样本的价值，以便在训练过程中选择最具信息量的样本。**
  - **通过选择最有价值的样本进行标记，主动学习可以在相对较少的标记数据的情况下构建高性能的模型**。这对于训练数据有限或者标记数据成本较高的任务尤为重要。通过主动学习，可以最大程度地利用有限的标记数据，从而提高模型的泛化能力和预测性能。

2. 事实上，**没有专家干涉，也可利用无标记样本数据的信息。**
若能观察到图中的未标记样本，则将很有把握地判别为正例。
![20230613213915](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613213915.png)
3. 让学习器不依赖外界交互，自动地利用未标记样本来提升学习性能，就是半监督学习（semi-supervised learning）
4. **要利用未标记样本，必然要做一些将未标记样本所揭示的数据分布信息与类别标记相联系的假设。**
   1. 聚类假设（cluster assumption）
   2. 流形假设（manifold assumption）
> 事实上，无论聚类假设还是流形假设，其本质都是“相似的样本拥有相似的输出”这个基本假设
5. **半监督学习可进一步划分为纯（pure）半监督学习和直推学习（transductive learning），前者假定训练数据中的未标记样本并非待预测的数据，而后者则假定学习过程中所考虑的未标记样本恰是待预测数据，学习的目的就是在这些未标记样本上获得最优泛化性能。**
一图胜千言：
![20230613214852](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613214852.png)
6. 生成式方法（Generative Methods）
- 生成式方法（Generative Methods）是直接基于生成式模型的方法。**此类方法假设所有数据（无论是否有标记）都是由同一个潜在的模型“生成”的。**这个假设使得我们能通过潜在模型的参数将未标记数据与学习目标联系起来，而未标记数据的标记则可看作模型的确实参数。
- **此类方法的区别主要在于生成式模型的假设，不同的模型假设将产生不同的方法。**
- 遗憾的是，**在现实任务中往往很难事先做出准确的模型假设，除非拥有充分可靠的领域知识。**
- 生成式方法常用的模型包括：
  1. 高斯混合模型（Gaussian Mixture Model，GMM）：GMM 是一种**概率模型，假设数据由多个高斯分布组合而成**。通过学习高斯分布的参数，GMM 可以生成新的数据样本。
  2. 隐马尔可夫模型（Hidden Markov Model，HMM）：HMM 是一种**时序模型，用于建模具有隐含状态的序列数据**。HMM 假设数据的生成过程是一个马尔可夫过程，通过学习转移矩阵和观测概率，HMM 可以生成与原始序列类似的新序列。
  3. 生成对抗网络（Generative Adversarial Network，GAN）：**GAN 是一种基于博弈论的生成模型，由生成器（Generator）和判别器（Discriminator）组成**。生成器试图生成逼真的数据样本，而判别器则试图区分生成的样本和真实样本。通过对抗训练，生成器逐渐学习生成高质量的数据样本。
  4. 变分自编码器（Variational Autoencoder，VAE）：**VAE 是一种基于神经网络的生成模型，通过学习数据样本的潜在变量分布，实现数据的生成和重构**。VAE 通过编码器将数据映射到潜在空间，再通过解码器将潜在变量映射回数据空间，从而生成新的数据样本。

7. 半监督支持向量机(Semi-Supervised Support Vector Machine, 简称S3VM)是支持向量机在半监督学习上的推广。**在不考虑未标记样本时，支持向量机试图找到最大间隔划分超平面，而在考虑未标记样本后，S3VM试图找到能将两类有标记样本分开，且穿过数据低密度区域的划分超平面**.
8. 半监督SVM的**基本思想是，通过在支持向量机的优化目标函数中引入无标签数据的约束，利用无标签数据的分布信息来提供额外的学习信号**。这样，模型可以通过利用无标签数据的分布特征来更好地学习分类边界，从而提高分类器的性能。
半监督SVM的训练过程包括以下步骤：
  1. 初始训练：使用有标签数据训练一个标准的支持向量机分类器。
  2. 标签传播：**使用该分类器对无标签数据进行预测，将预测结果作为无标签数据的伪标签。**
  3. 重新训练：将有标签数据和带有伪标签的无标签数据合并，重新训练支持向量机分类器。
  4. 重复迭代：重复执行步骤2和步骤3，直到达到一定的迭代次数或收敛条件。
通过不断迭代的过程，半监督SVM可以逐渐利用无标签数据的信息进行学习，从而提高分类器的泛化能力。
半监督SVM的优点在于可以利用未标记数据来提供额外的信息，从而扩大训练数据集，改善模型性能。它尤其适用于标记数据稀缺或者标记过程代价较高的情况。然而，**半监督SVM也面临一些挑战，如如何准确传播标签、无标签数据中的噪声以及类别不平衡等问题**。因此，在使用半监督SVM时需要谨慎处理这些问题，以获得更好的结果。

9. 图半监督学习（Graph Semi-Supervised Learning）
- **给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图中一个结点，若两个样本之间的相似度很高(或相关性很强)，则对应的结点之间存在一条边, 边的“强度”(strength)正比于样本之间的相似度(或相关性)**。我们可将有标记样本所对应的结点想象为染过色，而未标记样本所对应的结点尚未染色。于是，半监督学习就对应于“颜色”在图上扩散或传播的过程。**由于一个图对应了一个矩阵，这就使得我们能基于矩阵运算来进行半监督学习算法的推导与分析.**
- 图半监督学习（Graph Semi-Supervised Learning）是一种机器学习方法，旨在利用图结构数据中的标记和未标记节点信息来进行分类或回归任务。**与传统的半监督学习方法相比，图半监督学习更适用于处理具有图结构的数据，例如社交网络、推荐系统、生物网络等。**
- 图半监督学习的**基本思想是将节点之间的连接关系转化为图结构，利用图中节点的拓扑结构和特征信息来推断未标记节点的标签**。以下是图半监督学习的关键要素和步骤：
  1. 图构建：将数据集中的节点和它们之间的连接关系表示为图结构。连接关系可以是节点之间的边、邻接矩阵或者相似性矩阵。
  2. 节点特征提取：为每个节点提取特征表示，这些特征可以是节点的属性、结构信息或其他相关信息。常用的特征提取方法包括节点属性传播、图卷积网络（Graph Convolutional Networks）等。
  3. 标记传播：**利用已标记节点的标签信息，通过图结构进行标签传播，将已标记节点的标签信息传递给未标记节点**。传播的方式可以是基于图的扩散算法，如拉普拉斯传播（Laplacian Propagation）或基于图卷积网络的方法。
  4. 学习模型：基于已标记和未标记节点的特征和标签信息，训练一个分类器或回归模型来进行预测。常用的模型包括支持向量机、随机森林、神经网络等。
- 图半监督学习的优点在于它能够利用图结构中的节点连接关系和特征信息来推断未标记节点的标签，从而提供更准确的预测结果。它对于处理具有图结构的数据和标记数据不充分的情况下特别有用。然而，**图半监督学习也面临一些挑战，如图噪声、图稀疏性、标签传播的准确性等问题。**
- 图半监督学习方法在概念上相当清晰，且易于通过对涉矩阵运算的分析来探索算法性质。但此类算法的缺陷也相当明显：
  - 存储开销
  - 由于构图过程中仅能考虑训练样本集，**难以判知新样本在图中的位置。**

10. 基于分歧的方法（Divergence-Based Methods）
与生成式方法、半监督SVM、图半监督学习等基于单学习器利用未标记数据不同，**基于分歧的方法（Divergence-Based Methods）使用多学习器，而学习器之间的“分歧”（disagreement）对未标记数据的利用至关重要**
11. 聚类是一种典型的无监督学习任务，然而在现实聚类任务中我们往往能获得一些额外的监督信息。于是可通过**半监督聚类（Semi-Supervised Clustering）**来利用监督信息以获得更好的聚类效果。
12. 聚类任务中获得的监督信息大致有两种，**第一种类型是“必连”（must-link）与“勿连”（cannot-link）约束，前者是指样本必属于同一个簇，后者是指样本必不属于同一个簇**
13. 阅读材料
![20230613225704](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613225704.png)
> 许多集成学习研究者认为：只要能使用多个学习器即可将弱学习器性能提升到极高，无须使用未标记样本；许多半监督学习研究者认为：只要能使用未标记样本即可将弱学习器性能提升到极高，无须使用多学习器。但这两种看法都有其局限.
14. 半监督学习**不仅仅可应用于半监督分类和聚类，也可用于半监督回归、降维等方面**
15. **确保半监督学习的泛化性能至少不差于仅利用有标记样本仍是一个未解决的问题**


---------------------------------------------------------
# 第十四章 概率图模型
## 14.1 先修知识
- 概率图模型（Probabilistic Graphical Model，PGM）
  - 概率图模型（Probabilistic Graphical Model，PGM）是一种用于建模概率关系的图结构模型。**它能够表示变量之间的依赖关系**，并且使用概率分布来描述这些依赖关系。
  - 概率图模型主要有两种类型：**贝叶斯网络（Bayesian Network）和马尔可夫随机场（Markov Random Field）。**
    1. **贝叶斯网络（Bayesian Network，也称为有向图模型）：贝叶斯网络使用有向无环图（DAG）表示变量之间的依赖关系**。节点表示随机变量，边表示变量之间的条件依赖关系。每个节点都与一个条件概率表相关联，该表描述了给定其父节点的取值时，该节点的取值的概率分布。**贝叶斯网络可以用来进行因果推断和概率推理**，常用于处理不确定性推理和决策问题。
    2. **马尔可夫随机场（Markov Random Field，也称为无向图模型）：马尔可夫随机场使用无向图表示变量之间的依赖关系**。图中的节点表示随机变量，边表示变量之间的相关性。每个节点与一个势函数（或能量函数）相关联，该函数度量了该节点与其邻居节点的关系。马尔可夫随机场主要用于建模联合概率分布，可以用于图像分割、图像处理、计算机视觉等领域。
  概率图模型具有许多优点，包括清晰的可视化表示、模型的简洁性和模块化性质。它们提供了一种灵活的框架，用于对复杂的概率关系建模和推理。概率图模型在机器学习、人工智能、统计学和模式识别等领域中被广泛应用，包括决策网络、隐马尔可夫模型和条件随机场等。

- 隐马尔可夫模型（Hidden Markov Model，HMM）
  - 隐马尔可夫模型（Hidden Markov Model，HMM）是一种用于**建模序列数据**的统计模型。它是马尔可夫模型的扩展，**适用于存在状态（观察不到）和观察（可见）之间的关系的问题。**
  - **HMM由两个基本要素组成：状态和观察**。状态是隐藏的，表示系统内部的一种特定状态；观察是可见的，表示我们能够观测到的数据。在HMM中，系统在不同的状态之间转换，并且每个状态生成特定的观察。
  - **HMM由以下几个部分组成**：
    1. 状态集合（State set）：包含系统可能的状态。记为S={$s_1, s_2, ..., s_n$}。
    2. 观察集合（Observation set）：包含可以观察到的数据。记为O={$o_1, o_2, ..., o_m$}。
    3. 状态转移概率矩阵（Transition probability matrix）：表示状态之间的转换概率。记为A={$a_{ij}$}，其中{$a_{ij}$}表示从状态$s_i$转移到状态$s_j$的概率。
    4. 观察概率矩阵（Observation probability matrix）：表示在给定状态下生成观察的概率。记为B={$b_{ij}$}，其中B={$b_{ij}$}表示在状态$s_i$生成观察$o_j$的概率。
    5. 初始状态概率向量（Initial state probability vector）：表示系统初始状态的概率分布。记为π={$π_i$}，其中$π_i$表示系统初始状态为$s_i$的概率。
  - **HMM有三个基本问题**：
    1. 评估问题（Evaluation problem）：给定模型λ=(A, B, π)和观察序列O，计算给定观察序列的概率P(O|λ)。
    2. 解码问题（Decoding problem）：给定模型λ=(A, B, π)和观察序列O，找到最可能的对应状态序列。
    3. 学习问题（Learning problem）：给定观察序列O，估计模型λ=(A, B, π)的参数。
  - HMM的应用非常广泛，特别适用于语音识别、自然语言处理、生物信息学和时间序列分析等领域。通过对状态和观察之间的关系进行建模，HMM可以用来预测未来的状态，进行序列标注和分类，以及进行序列生成等任务。

- 马尔可夫随机场（Markov Random Field，MRF）
  - **马尔可夫随机场（Markov Random Field，MRF）是一种用于建模联合概率分布的无向图模型**。它描述了一组变量之间的相关性，并且基于马尔可夫性质，即给定一个变量集合的条件下，**每个变量只依赖于其邻居节点的取值**。
  - MRF由一个无向图表示，图中的节点表示随机变量，边表示变量之间的相关性。**与有向图模型（如贝叶斯网络）不同，MRF没有明确的因果关系，变量之间的依赖关系是对称的**。
  - MRF中的变量可以分为两类：
    1. 可见变量（Observable Variables）：表示我们能够直接观察到的变量。
    2. 隐藏变量（Hidden Variables）：表示我们无法直接观察到的变量。
  - **MRF使用势函数（Potential Function）来度量变量之间的相关性**。势函数定义在**图中的团（Clique）上，一个团是指图中完全连接的一组节点**。势函数通常表示为指数函数的形式，其中指数的值与团的配置相关。
  - MRF的联合概率分布可以通过对势函数的归一化常数进行标准化来计算。给定MRF的势函数和变量的取值，可以计算出整个网络的联合概率分布。
  - MRF可以用于许多任务，包括图像处理、计算机视觉、模式识别和统计推断等。例如，在图像处理中，MRF被广泛用于图像分割、图像去噪和图像恢复等任务，其中变量通常表示图像的像素或区域。
  - **对于MRF，推断问题是一个重要的任务**，包括计算变量的边缘概率、最大后验概率估计和找到具有最高概率的变量配置。**常用的推断算法包括信念传播（Belief Propagation）、Gibbs采样和变分推断等**。
  - 总而言之，马尔可夫随机场是一种强大的概率图模型，能够用于建模变量之间的相关性。它具有广泛的应用领域，并在许多机器学习和人工智能任务中发挥重要作用。

---------------------------------------------------
1. 概率模型（probabilistic model）提供了一种描述框架，将学习任务归结于计算变量的概率分布。**在概率模型中，利用已知变量推测未知变量的分布称为“推断”（inference）**，其核心是如何基于可观测变量推测出未知变量的**条件分布**。**直接利用概率求和规则消去变量R显然不可行，因为即便每个变量仅有两种取值的简单问题，其复杂度已至少是$O(2^{|Y|+|R|})$.
2. 概率图模型（probabilistic graphical model）是一类用图来表达变量相关关系的概率模型。它以图为表示工具，最常见的是用一个结点表示一个或一组随机变量，节点之间的边表示变量间的概率相关关系，即“变量关系图”
3. 根据边的性质不同，概率图模型可大致分为两类：
   1. 使用**有向无环图**表示变量间的**依赖关系**，称为**有向图模型或贝叶斯网（Bayesian netword）**
   2. 使用**无向图**表示变量间的**相关关系**，称为**无向图模型或马尔可夫网（Markov network）**
4. **隐马尔可夫模型（Hidden Markov Model，简称HMM）是结构最简单的动态贝叶斯网（dynamic Bayesian network），这是一种著名的有向图模型，主要用于时序数据建模**
5. **隐马尔可夫用到了所谓的“马尔科夫链”（Markov chain），即：系统下一时刻的状态仅由当前状态决定，不依赖于以往的任何状态**
6. 隐马尔可夫模型的三个基本问题：
![20230614100417](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614100417.png)
> 值得庆幸的是，基于条件独立性，隐马尔可夫模型的这三个问题均能被高效求解

7. 马尔可夫随机场（Markov Random Field，MRF）是典型的马尔可夫网，这是一种著名的**无向图模型**。图中每个结点表示一个或一组变量，节点之间的边表示两个变量之间的**依赖关系**。马尔可夫随机场有一组**势函数（potential functions），亦称“因子”（factor），这是定义在变量子集上的非负实函数，主要用于定义概分布函数**.对于图中结点的一个子集，**若其中任意两结点间都有边连接，则称该结点子集为一个“团”（clique）**.若在一个团中加入另外任何一个结点都不再形成团，则称该团为“极大团”（maximal clique）；**换言之，极大团就是不能被其他团所包含的团**。

TODO


-----------------------------------------------------------
# 第十五章 规则学习
1. “规则学习”（rule learning）是从训练数据中学习出一组能用于对未见示例进行判别的规则。**规则学习中的“规则”是狭义的，事实上约定俗成地省略了“逻辑”二字**。形式化地看，一条规则形如：
![20230614221645](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614221645.png)
> 其中逻辑蕴含符号“$\leftarrow$”右边部分称为“规则体”（body），表示该条规则的前提，左边部分称为“规则头”（head），表示该条规则的结果。

2. 规则体是由逻辑文字（literal）$f_k$组成的合取式（conjunction），其中合取符号“∧”用来表示“并且”。每个文字$f_k$都是对示例属性进行检验的布尔表达式。
3. 规则学习有更好的可解释性
4. **数理逻辑具有极强的表达能力**。因此，规则学习能更自然地在学习过程中引入领域知识。
5. 符合某条规则地样本称为被该规则“覆盖”（cover）。**需要注意的是，被正例规则覆盖的样本是正例，但没被正例规则覆盖的未必不是正例；只有被反例规则覆盖的样本才是反例**。
6. **显然，规则集合中的每条规则都可看作一个子模型，规则集合是这些子模型的一个集成**。
7. **当同一个示例被判别结果不同的多条规则覆盖时，称发生了“冲突”（conflict），解决冲突的办法称为“冲突消解”（conflict resolution）**。常用的冲突消解策略有投票法、排序法、元规则法等。
8. **从训练集学得的规则集合也许不能覆盖所有可能的未见示例，这种情况在属性数目很多时常出现。因此，规则学习算法通常会设置一条“默认规则”（default rule），由它来处理规则集合未覆盖的样本**。
9. 从形式语言表达能力而言，规则可分为两类：**“命题规则”（propositional rule）和“一阶规则”（first-order rule）**.
- 显然，**一阶规则能表达复杂的关系**，因此也被称为“关系型规则”（relational rule）
- 显然，从形式语言系统的角度来看，**命题规则是一阶规则的特例，因此一阶规则的学习比命题规则要复杂得多**
10. **规则学习的目标是产生一个能覆盖尽可能多的样例的规则集。最直接的做法是“序贯覆盖”(sequentialcovering)，即逐条归纳**：在训练集上每学到一条规则，就将该规则覆盖的训练样例去除，然后以剩下的训练样例组成训练集重复上述过程。由于每次只处理一部分数据，因此**也被称为“分治”(separate- and conquer)策略**.
11. **序贯覆盖（Sequential Covering）**是一种基于规则学习的机器学习方法，用于从数据中提取一组规则来描述数据之间的关系。它的目标是**生成一组简单而准确的规则，每个规则都能覆盖尽可能多的训练实例**。
- 序贯覆盖通常用于**监督学习**任务，其中训练数据包含输入特征和相应的输出标签。它逐步生成规则，每次生成一条规则并移除该规则覆盖的实例，然后再基于更新后的数据继续生成下一条规则。**这种逐步生成的过程可以确保每个规则都专注于不同的实例子集**。
- 下面是序贯覆盖的基本步骤：
  1. 初始化：准备训练数据集和一个初始规则集。
  2. 选择规则：从剩余的未覆盖实例中选择一个规则进行生成。**通常采用启发式方法**，如基于信息增益或基尼指数的特征选择来确定生成规则的顺序。
  3. 生成规则：针对选择的规则，根据数据的特征和标签生成规则的条件和结论。生成的规则应该足够简单、具有高准确性，并且能够覆盖尽可能多的实例。
  4. 移除实例：将被生成的规则覆盖的实例从数据集中移除，以确保下一个规则能够关注到其他未覆盖的实例。
  5. 终止条件：**检查终止条件，例如达到指定的规则数量、无法生成更多规则或覆盖率达到一定阈值等**。
  6. 重复生成：如果终止条件未满足，返回步骤2，选择下一个规则进行生成。
- 序贯覆盖的优点之一是生成的规则简单且易于解释，因为每个规则都是基于部分实例生成的，并且可以根据规则的覆盖情况来评估其重要性。然而，**序贯覆盖也有一些挑战，例如处理大规模数据集时的效率问题以及规则之间的重叠和冲突**。在实际应用中，可以使用一些技术来改进序贯覆盖算法，例如**剪枝、规则合并和后剪枝等**。

12. **一阶规则学习（First-Order Rule Learning）**是机器学习领域中的一种方法，用于从数据中学习一阶逻辑规则。与传统的规则学习方法相比，一阶规则学习能够处理更丰富和复杂的关系，包括对象之间的关联和属性之间的约束。
**一阶逻辑是一种用于描述关系和约束的形式化语言，它包含谓词（描述对象的性质或关系）、变量（代表对象）、常量（具体的对象）和逻辑连接词（如AND、OR、NOT等）。一阶规则学习的目标是通过学习从输入特征到输出标签的一阶逻辑规则来进行分类、预测或推理**。
以下是一阶规则学习的一般步骤：
  1. 数据准备：收集和准备用于学习的数据集，包括一组特征和相应的标签。
  2. 特征工程：根据任务和数据的特点，对输入特征进行处理和选择，以提取有意义的一阶逻辑约束。
  3. 规则生成：使用一阶规则学习算法从数据集中生成一阶逻辑规则。这通常包括确定规则的结构、选择谓词和变量，并确定规则的条件和结论。
  4. 规则评估：评估生成的规则对于新数据的预测能力。可以使用交叉验证或保留一部分数据进行评估。
  5. 规则应用：将生成的规则应用于新的未知实例，进行分类、预测或推理。
**一阶规则学习的优点之一是能够处理复杂的关系和约束，因为一阶逻辑具有丰富的表达能力。它可以应用于多个领域，包括自然语言处理、知识图谱构建、关系抽取等任务。然而，一阶规则学习也面临一些挑战，如处理大规模数据集的计算复杂性和过拟合问题**。近年来，一阶规则学习领域涌现出一些高效的算法和技术，如基于逻辑编程的方法、约束学习和归纳逻辑编程等，以改善一阶规则学习的效果和效率。

13. **归纳逻辑程序设计（Inductive Logic Programming，简称ILP）**是一种机器学习方法，**结合了一阶逻辑（First-Order Logic）和逻辑程序设计的思想，用于从数据中归纳出一组逻辑程序规则。ILP的目标是通过学习逻辑规则来解决基于背景知识的逻辑程序归纳问题，这些规则可以直接在逻辑程序设计语言（如PROLOG）中使用**。
- 在ILP中，背景知识以逻辑程序的形式给出，它包括一组事实、规则和约束，描述了问题领域的先验知识。ILP的任务是根据给定的背景知识和训练数据集，学习出一组逻辑规则，使得这些规则能够解释已知的数据，并具有一定的泛化能力。
- ILP的基本思想是通过在逻辑程序中插入变量和函数，使得规则可以通过归纳推理从例子中学习出来。ILP算法通常包括以下步骤：
  1. 定义背景知识：将问题的背景知识表示为逻辑程序，包括事实、规则和约束。
  2. 数据准备：收集和准备用于学习的训练数据集，其中每个示例由输入和输出标签组成。
  3. 候选规则生成：基于背景知识，生成候选的逻辑规则，这些规则包含变量和函数，以便进行归纳推理。
  4. 归纳推理：使用归纳推理算法，将候选规则与训练数据集进行匹配和推理，以找到满足数据的规则。
  5. 规则评估和修剪：评估生成的规则对于训练数据和背景知识的拟合程度，并进行修剪或组合，以提高规则的泛化能力。
  6. 输出规则：输出学得的逻辑规则，这些规则可以直接在逻辑程序设计语言中使用，例如PROLOG。
- 归纳逻辑程序设计具有强大的表达能力，能够处理复杂的关系和约束。它在领域知识比较充分、问题具有明确的逻辑结构的任务中表现出色，例如自然语言处理、关系抽取、知识图谱构建等。然而，由于ILP的计算复杂性较高，处理大规模数据和复杂问题时可能面临挑战。因此，研究者们一直在探索和改进ILP算法，以提高其效率和可扩展性。


--------------------------------------------------------
# 第十六章 强化学习
1. 强化学习（reinforcement learning）
  - 强化学习（Reinforcement Learning，RL）是机器学习的一个分支，**用于解决智能体（Agent）在与环境进行交互的过程中，通过试错和学习来获得最优行为策略的问题**。强化学习是一种通过奖励信号来引导学习的方法，智能体通过与环境的交互，通过尝试不同的行动并接收环境的反馈来学习如何在给定的环境中做出最佳的决策。
  - 强化学习涉及以下几个核心元素：
    1. 环境（Environment）：强化学习中智能体所处的外部环境，它包含智能体**可以观察到的状态和智能体可以采取的动作**。
    2. 智能体（Agent）：强化学习的学习主体，通过观察环境的状态，选择合适的动作以最大化累积奖励。
    3. 状态（State）：环境的观察值，用于描述环境的特定情况。
    4. 动作（Action）：智能体在给定状态下可以执行的操作或决策。
    5. 奖励（Reward）：环境在智能体执行动作后**提供的反馈信号，用于指导智能体的学习过程。奖励可以是正向的（鼓励行为）或负向的（惩罚行为）**。
  - 强化学习的目标是通过学习最优的策略，使智能体在环境中**获得最大的累积奖励。学习过程通常基于价值函数（Value Function）或策略（Policy）的优化**。价值函数用于评估在给定状态下采取特定动作的**长期回报，而策略定义了智能体在给定状态下应该采取的动作**。
  - 强化学习算法可以基于**模型（Model-Based）或无模型（Model-Free）进行分类**。模型基础的方法通过学习环境的模型（包括状态转移和奖励函数）来做出决策。无模型的方法则直接从与环境的交互中学习，通过尝试和错误来更新价值函数或策略。
  - 强化学习在许多领域有广泛应用，包括机器人控制、自动驾驶、游戏玩法、资源管理和金融交易等。著名的强化学习算法包括Q-learning、SARSA、深度强化学习（Deep Reinforcement Learning）和策略梯度（Policy Gradient）等。
  - **总而言之，强化学习是一种通过试错和学习来获得最优行为策略的机器学习方法。它通过智能体与环境的交互和奖励信号来引导学习，以实现在给定环境下最大化累积奖励的目标**。

2. 马尔可夫决策过程（Markov Decision Process，MDP）
- **马尔可夫决策过程（Markov Decision Process，MDP）是一种用于建模序贯决策问题的数学框架**。它基于马尔可夫性质，**描述了在随机环境中，智能体通过采取不同的动作来达到目标的决策过程**。
- MDP由五个核心要素组成：
  1. 状态（State）：表示智能体所处的环境状态。**在每个时间步，智能体通过观察当前状态来做出决策**。
  2. 动作（Action）：表示智能体可以采取的行动或决策。**在给定状态下，智能体可以选择执行的动作集合**。
  3. 转移概率（Transition Probability）：**表示从一个状态到另一个状态的转移概率**。给定当前状态和采取的动作，转移概率描述了下一个状态的**可能性分布**。
  4. 奖励（Reward）：表示智能体在状态转移过程中获得的**即时反馈**。奖励可以是正向的（鼓励行为）或负向的（惩罚行为）。
  5. 折扣因子（Discount Factor）：**用于权衡当前奖励和未来奖励的重要性。折扣因子决定了未来奖励对智能体决策的相对重要性**。
- 基于这些要素，MDP的**目标是找到一个最优策略（Policy），使智能体在给定状态下能够选择最佳的动作以最大化累积奖励**。最优策略可以通过价值函数（Value Function）来衡量，价值函数表示从给定状态开始，智能体在执行最优策略下可以获得的长期回报。
- MDP的求解可以使用**动态规划方法，其中常用的算法包括值迭代（Value Iteration）和策略迭代（Policy Iteration）**。值迭代通过迭代更新价值函数来逐步逼近最优策略，而策略迭代则通过反复改进策略和更新价值函数来获得最优策略。**策略迭代算法在每次改进策略后都需重新进行策略评估，这通常比较耗时。策略改进与值函数的改进是一致的，因此可将策略改进视为值函数的改善**
- 马尔可夫决策过程在强化学习中具有重要的地位，被广泛应用于智能体在复杂环境中进行决策和规划的问题，如机器人控制、自动驾驶和资源管理等领域。

3. 总之，在环境中状态的转移、奖赏的返回是不受机器控制的，**机器只能通过选择要执行的动作来影响环境，也只能通过观察转移后的状态和返回的奖赏来感知环境**
4. 机器要做的是通过在环境中不断尝试而学得一个“策略”（policy）$\pi$。策略有两种表示办法：一种是将策略表示为函数$\pi : X \rightarrow A$，确定性策略常用这种表示；另一种是概率表示$\pi : X x A \rightarrow \mathbb{R}$，随机性策略常用这种表示。
5. **浅谈强化学习与监督学习的差别**
![20230614150202](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614150202.png)

## 16.1 K-摇臂赌博机（K-Armed Bandit），亦称“K-摇臂老虎机”
- 探索与利用
  - 强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：**最大化单步奖赏，即仅考虑一步操作。欲最大化单步奖赏需考虑两个方面**：
    - 每个动作带来的奖赏
    - 要执行奖赏最大的动作
  若每个动作对应的奖赏是一个确定值，那么尝试一遍所有的动作便能找出奖赏最大的动作。**然而，更一般的情形是，一个动作的奖赏值是来自于一个概率分布，仅通过一次尝试并不能确切地获得平均奖赏值**.
- **这样的单步强化学习任务对应了一个理论模型，即“K-摇臂赌博机”（K-armed bandit）**。K-摇臂赌博机有K个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。
- **若仅为获知每个摇臂的期望奖赏，则可采用仅探索”（exploration-only）**法：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。**若仅为执行奖赏最大的动作，则可采用“仅利用”（exploitation-only）**法：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。
> 显然，“仅探索”法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；“仅利用”法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。

- 事实上，**“探索”(即估计摇臂的优劣)和“利用”(即选择当前最优摇臂)这两者是矛盾的**，因为尝试次数(即总投币数)有限，加强了一方则会自然削弱另一方，**这就是强化学习所面临的“探索利用窘境”(Exploration-Exploitation dilemma)**。显然，**欲累积奖赏最大，则必须在探索与利用之间达成较好的折中.**

6. K-摇臂赌博机（K-Armed Bandit）是强化学习中的一个经典问题。**它模拟了在面临多个选择（摇臂）时，如何在有限的资源和不确定性环境下进行决策的情境**。
- K-摇臂赌博机的设定如下：
  1. 摇臂（Arms）：有K个摇臂可供选择，每个摇臂**都有一个固定的概率分布**，表示在选择该摇臂时获得奖励的概率分布。
  2. 奖励（Reward）：在每次选择摇臂后，根据所选择摇臂的概率分布随机产生一个奖励，奖励可以是实数值或离散值。
  3. 动作选择（Action Selection）：在每个时间步，智能体需要选择一个摇臂进行操作。选择哪个摇臂是智能体的决策。
  4. 目标：智能体的目标是通过与摇臂的交互累积获得最大的奖励。
- **在K-摇臂赌博机问题中，智能体需要在探索（Exploration）和利用（Exploitation）之间做出权衡。探索是指尝试选择未知的摇臂以了解它们的概率分布和奖励情况，以便获取更准确的信息。利用是指选择已知的高奖励摇臂，以最大化当前获得的奖励**。
- 解决K-摇臂赌博机问题的关键是找到一个合适的策略，即在探索和利用之间找到平衡点。常用的策略包括：
  1. ε-贪心策略（ε-Greedy）：以1-ε的概率选择当前奖励最高的摇臂，以ε的概率选择随机摇臂，以促进探索。
  2. 上限置信界（Upper Confidence Bound，UCB）：根据奖励的置信界，选择具有最高上界的摇臂，以平衡探索和利用。
  3. 梯度赌博算法（Gradient Bandit Algorithm）：使用梯度信息来调整选择每个摇臂的概率，以最大化奖励的期望。
- 这些策略都有不同的权衡，探索能力强的策略可能会获得更多信息，但可能在短期内获得较低的奖励；而利用能力强的策略可能会在短期内获得较高的奖励，但可能会错过更高奖励的摇臂。
- K-摇臂赌博机问题作为一个简单但具有挑战性的强化学习问题，可以帮助我们理解和研究在有限资源和不确定环境下的决策过程，并为更复杂的强化学习问题提供基础。

7. $\epsilon$-贪心法基于一个概率来对探索和利用进行折中：每次尝试时，以$\epsilon$的概率进行探索，即以均匀概率随机选取一个摇臂；以$1 - \epsilon$的概率进行利用，即选择当前平均奖赏最高的摇臂(若有多个，则随机选取一个).
> 若摇臂奖赏的不确定性较大，例如概率分布较宽时，则需更多的探索，此时需要较大的$\epsilon$值；若摇臂的不确定性较小，例如概率分布较集中时，则少量的尝试就能很好地近似真实奖赏，此时需要的$\epsilon$较小。通常令$\epsilon$取一个较小的常数，如0.1或0.01。**然而，若尝试次数非常大，那么在一段时间后，摇臂的奖赏都能很好地近似出来，不再需要探索，这种情形下可让$\epsilon$随着尝试次数的增加而逐渐减小**，例如令$\epsilon = 1/\sqrt{t}$.

8. Softmax
- Softmax算法基于**当前已知的摇臂平均奖赏**来对探索和利用进行折中。**若各摇臂的平均奖赏相当，则选取各摇臂的概率也相当；若某些摇臂的平均奖赏明显高于其他摇臂,则它们被选取的概率也明显更高.**
![20230614155654](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614155654.png)
9. 对于离散状态空间、离散动作空间上的**多步强化学习任务，一种直接的办法是将每个状态上动作的选择看作一个K-摇臂赌博机问题，用强化学习任务的累积奖赏来代替K-摇臂赌博机算法中的奖赏函数**，即可将赌博机算法用于每个状态：对每个状态分别记录各动作的尝试次数、当前平均累积奖赏等信息，基于赌博机算法选择要尝试的动作。**然而这样的做法有很多局限，因为它没有考虑强化学习任务马尔可夫决策过程的结构**。若能有效考虑马尔可夫决策过程的特性，则可有更聪明的办法.
10. **考虑多步强化学习任务，暂且先假定任务对应的马尔可夫决策过程四元组E = (X，A，P，R)均为已知，这样的情形称为“模型已知”**，即机器已对环境进行了建模，能在机器内部模拟出与环境相同或近似的状况。**在已知模型的环境中学习称为“有模型学习”(model-based learning)**.
11. 在现实的强化学习任务中，环境的转移概率、奖赏函数往往很难得知，甚至很难知道环境中一共有多少状态。**若学习算法不依赖于环境建模，则称为“免模型学习”(model-free learning)，亦称“无模型学习”，这比有模型学习要困难得多**.
12. **在免模型情形下，策略迭代算法首先遇到的问题是策略无法评估，这是由于模型未知而导致无法做全概率展开**。此时，只能通过在环境中执行选择的动作，来观察转移的状态和得到的奖赏。受K摇臂赌博机的启发，**一种直接的策略评估替代方法是多次“采样”，然后求取平均累积奖赏来作为期望累积奖赏的近似，这称为蒙特卡罗强化学习**。由于采样必须为有限次数，因此该方法更适合于使用T步累积奖赏的强化学习任务.
13. **蒙特卡罗强化学习（Monte Carlo Reinforcement Learning）是一种基于样本采样的强化学习方法。它通过与环境的交互来获取经验数据，并利用这些数据进行策略评估和策略改进**。
- 蒙特卡罗强化学习的核心思想是基于回报样本的累积来评估策略的好坏。**具体而言，它通过执行一系列完整的回合（Episode）来采样轨迹数据**。在每个回合中，智能体与环境进行交互，根据当前的策略选择动作，并观察环境的反馈（奖励和下一个状态）。通过多次回合的交互，蒙特卡罗方法可以得到一组回合样本。
- 利用这些回合样本，蒙特卡罗强化学习主要包含两个步骤：**策略评估和策略改进**。
  1. 策略评估（Policy Evaluation）：在策略评估中，通过对回合样本的回报进行累积，估计每个状态的值函数或状态动作对值函数。一种常用的方法是使用蒙特卡罗预测（Monte Carlo Prediction），根据回合样本计算状态值函数或状态动作对值函数的平均回报。
  2. 策略改进（Policy Improvement）：在策略改进中，基于估计的值函数，可以对策略进行改进。常用的方法是使用ε-贪心策略（ε-Greedy Policy），以一定概率选择当前估计值最高的动作，以提高利用能力；同时，以一定概率选择随机动作，以增加探索能力。策略改进可以根据当前值函数的估计，生成新的策略。
- 通过反复进行策略评估和策略改进，蒙特卡罗强化学习逐渐优化策略，并逐步收敛到最优策略。
- **蒙特卡罗强化学习的优点是它不需要对环境的转移概率进行假设，只利用交互的样本数据进行学习。然而，由于需要采样大量的回合样本，蒙特卡罗方法通常具有较高的计算成本和样本效率。此外，它在处理连续状态和动作空间的问题上存在一定的挑战**。
- 总之，蒙特卡罗强化学习是一种基于样本采样的强化学习方法，通过采样回合样本来评估和改进策略。它是强化学习中重要的方法之一，为解决复杂的强化学习问题提供了一种有效的学习框架。

14. **时序差分学习（Temporal Difference Learning）是一种强化学习方法，它结合了动态规划和蒙特卡罗方法的优点。时序差分学习用于估计值函数或值函数的更新，通过观察到的即时奖励和预测的未来奖励之间的差异来进行学习**。
- 在时序差分学习中，智能体通过与环境的交互来获取经验数据。在每个时间步，智能体观察当前状态，选择动作并执行，然后观察环境的反馈（奖励）和下一个状态。智能体使用这些信息来**更新值函数的估计**。
- 时序差分学习使用**贝尔曼方程（Bellman equation）作为基本的更新规则**。贝尔曼方程描述了值函数的递归关系，将当前状态的值与下一个状态的值之间建立了联系。
- 具体而言，**时序差分学习中最常用的方法是Q-learning**。Q-learning使用一个Q值函数（动作值函数）来估计在给定状态下执行每个动作的价值。在每个时间步，Q-learning通过以下更新规则来更新Q值函数的估计：
$$Q(s, a) = Q(s, a) + \alpha [r + \gamma max_a' Q(s', a') - Q(s, a)]$$
> 其中，Q(s, a)表示在状态s下执行动作a的Q值，r表示即时奖励，s'表示下一个状态，a'表示下一个状态下的最优动作，$\alpha$表示学习率（用于控制更新幅度），$\gamma$表示折扣因子（用于平衡即时奖励和未来奖励的重要性）。
- Q-learning通过迭代地更新Q值函数，逐渐收敛到最优的Q值函数。一旦获得最优的Q值函数，智能体可以根据该函数选择具有最高Q值的动作作为策略。
- **时序差分学习的优点是它可以在每个时间步进行更新，不需要等待整个回合的结束，因此具有较高的样本效率。它还能够处理连续状态和动作空间，并且能够在未知环境中进行在线学习**。
- 总之，时序差分学习是一种强化学习方法，通过观察到的即时奖励和预测的未来奖励之间的差异来更新值函数的估计。Q-learning是时序差分学习的一种常用方法，通过迭代更新Q值函数来实现值函数的学习和策略的改进。

15. **值函数近似是一种在强化学习中用于估计值函数的方法。它的主要思想是通过函数逼近器（如线性函数、神经网络等）来近似值函数，而不是显式地存储和计算所有可能状态的值**。
- 在强化学习中，值函数用于估计每个状态或状态动作对的价值，表示在当前状态下采取行动的预期回报。然而，对于大规模状态空间或连续状态空间的问题，显式存储和计算值函数是不可行的。值函数近似通过使用函数逼近器来解决这个问题，将状态空间映射到一个参数化的函数空间中。
- 值函数近似的基本步骤如下：
  1. 选择一个函数逼近器：值函数近似使用函数逼近器来估计值函数。常见的函数逼近器包括线性函数、多项式函数、神经网络等。函数逼近器的选择取决于具体问题的性质和复杂度。
  2. 定义特征表示：为了进行值函数近似，需要将状态或状态动作对映射到函数逼近器所能接受的输入格式。通常，需要定义一组特征表示来描述状态或状态动作对的属性。这些特征可以是原始状态的函数变换、经验数据的统计特征等。
  3. 参数估计：使用强化学习算法，根据采集的样本数据，对函数逼近器的参数进行估计。常见的算法包括梯度下降、随机梯度下降、Q-learning等。通过与环境的交互，根据奖励信号和下一个状态的值函数估计，更新函数逼近器的参数。
  4. 策略改进：根据近似的值函数，可以使用贪心策略或其他策略选择方法来改进智能体的策略。根据值函数近似的结果，智能体可以选择具有最高估计值的动作作为策略。
- 值函数近似的优点是它能够处理大规模状态空间或连续状态空间的问题，节省了存储和计算的开销。然而，它也面临着一些挑战，如函数逼近器的选择、特征表示的设计和参数估计的稳定性等。
- **总结而言，值函数近似是一种通过使用函数逼近器来估计值函数的方法。它通过将状态映射到函数空间中，解决了大规模状态空间或连续状态空间问题中的存储和计算困难**。

16. **模仿学习（Imitation Learning）是一种强化学习的方法，旨在通过观察和模仿专家（或者已经解决任务的人）的行为来学习任务的策略**。模仿学习的目标是从专家的示范中学习到一个能够执行类似行为的策略，而无需进行探索和试错。
- 在模仿学习中，通常存在两种主要方法：**直接模仿学习和逆强化学习**。
  - 直接模仿学习（Direct Imitation Learning）：
    - 直接模仿学习是一种简单直接的模仿方法，它通过收集专家演示的样本数据，然后直接使用这些数据来训练一个模型，例如神经网络。模型会尽量复制专家示范的行为。在训练过程中，模型学习将输入状态映射到输出动作的映射关系，以实现与专家类似的策略。**直接模仿学习可以使用监督学习的方法来进行训练，其中输入是状态，输出是动作**。
    - **直接模仿学习的优点是简单直接，容易实现。然而，它也面临一些挑战。例如，直接模仿学习可能会受到专家演示中的噪声和偏差的影响，导致模型学到错误的策略。此外，直接模仿学习无法解决探索问题，因为它仅仅是复制专家的行为而不具备主动探索新策略的能力**。
  - 逆强化学习（Inverse Reinforcement Learning）：
    - **逆强化学习是一种基于观察者行为来推断其背后的奖励函数的方法**。在逆强化学习中，智能体试图从专家示范中学习到任务的奖励函数，然后根据这个奖励函数来执行任务。**逆强化学习的目标是理解专家示范的动机和目标，而不仅仅是模仿其行为**。
    - 逆强化学习的基本思想是通过比较专家示范的行为与候选奖励函数生成的行为来推断奖励函数。通过迭代的过程，逆强化学习算法可以找到与专家示范最一致的奖励函数。**一旦获得了奖励函数，智能体可以使用强化学习方法来学习最优策略**。
    - **逆强化学习的优点是它能够从专家示范中推断出更深层次的意图和目标，使得智能体能够更加灵活地适应环境和任务变化。然而，逆强化学习也存在一些挑战，例如推断奖励函数的不确定性、计算复杂度和过拟合等问题**。

总结而言，**模仿学习是一种通过观察和模仿专家的行为来学习任务策略的方法。直接模仿学习直接使用专家示范来训练模型，而逆强化学习通过推断奖励函数来学习专家的目标和动机**。这些方法在实际应用中具有各自的优缺点，可以根据具体问题选择合适的方法。