--------------------------------------------------------
# 第七章 贝叶斯分类器
1. 对于分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判所示来选择最优的类别标记。
2. 贝叶斯判定准则（Bayes decision rule）：为最小化总体风险，只需在每个样本上选择那个能使条件风险R(c|x)最小的类别标记。
3. 为便于讨论，假设所有属性均为离散型。对连续属性，可将概率质量函数P(·)换成概率密度函数p(·)
![20230611161623](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230611161623.png)
![20230611161652](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230611161652.png)
4. [贝叶斯估计、最大似然估计、最大后验概率估计](https://corner430.github.io/2023/04/03/Maximum-Likelihood-Estimation-Bayesian-Estimation-and-Maximum-A-Posteriori-Estimation/)
![20230611163211](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230611163211.png)
5. 基于有限训练样本直接估计联合概率，在计算上将会遭遇组合爆炸问题，在数据上将会遭遇样本稀疏问题；属性数越多，问题越严重。
6. 不难发现，基于贝叶斯公式来估计后验概率P(c|x)的主要困难在于：类条件概率P(x|c)是所有属性上的联合概率，难以从有限的训练样本直接估计而得。为避开这个障碍，朴素贝叶斯分类器(naive Bayes classifier)采用了**“属性条件独立性假设”(attribute conditional independence assumption)**：对已知类别，假设所有属性相互独立。换言之，假设每个属性独立地对分类结果生影响。
7. 若某个属性值在**训练集中没有与某个类同时出现过**，则直接进行概率估计，之后进行判别将出现问题。为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，在估计概率值时通常要进行“平滑”（smoothing），常用“拉普拉斯修正”（Laplacian correction）
8. 显然拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零的问题，并且在训练集变大时，修正过程所引入的先验（prior）的影响也会逐渐变得可忽略，使得估值渐趋向于实际概率值。
9. 拉普拉斯修正实际上假设了属性值与类别均匀分布，这是在朴素贝叶斯学习过程中额外引入了关于数据的先验。
10. 朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立。于是，人们尝试对属性条件独立性假设进行一定程度的放松，由此产生了一类为**“半朴素贝叶斯分类器”（semi-naive Bayes classifiers）**的学习方法。
- 半朴素贝叶斯分类器的基本想法是**适当考虑一部分属性间的相互依赖信息**，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。 
  - 独依赖估计（One-Dependent Estimator，简称ODE）是半朴素贝叶斯分类器最常用的一种策略。顾名思义，**所谓“独依赖”就是假设每个属性在类别之外最多仅依赖于一个其他属性**。
![20230612162450](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612162450.png)
![20230612162514](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612162514.png)
    - TAN（Tree Augmented naive Bayes）
![20230612162642](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612162642.png)
    - AODE（Averaged One-Dependent Estimator）是一种基于集成学习机制、更为强大的独依赖分类器。与SPODE通过模型选择确定超父属性不同、AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够数据支撑的SPODE集成起来作为最终结果。
  - 贝叶斯网（Bayesian network）亦称“信念网”（belief network），它借助有向无环图（Directed Acyclic Graph，简称DAG）来刻画属性之间的依赖关系，并使用条件概率表（Conditional Probability Table，简称CRT）来描述属性的联合概率分布。
  - <font color="#dd0000">贝叶斯结构有效地表达了属性间的条件独立性</font><br />
![20230612163728](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612163728.png)
![20230612163808](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612163808.png)
11. 若网络结构已知，即属性间的依赖关系已知，则贝叶斯网的学习过程相对简单，只需通过对训练样本“计数”，估计出每个节点的条件概率表即可。但在现实应用中我们往往并不知晓网格结构，**于是贝叶斯网学习的首要任务就是根据训练数据集来找出结构最“恰当”的贝叶斯网。** **“评分搜索”**是求解这一问题的常用办法。
12. EM算法（Expectation Maximization Algorithm）是一种迭代优化算法，**用于解决在概率统计中的一类问题，特别是在存在隐变量（或未观测变量）的概率模型中的参数估计问题。**
EM算法的核心思想是通过**迭代地进行两个步骤：E步骤（Expectation Step）和M步骤（Maximization Step），来逐步优化参数估计。**
- 以下是EM算法的基本步骤：
  1. **初始化参数**: 首先需要对模型的参数进行初始化。
  2. **E步骤（Expectation Step）**: 在E步骤中，根据当前的参数估计，计算隐变量的后验概率分布。这里的隐变量指的是未观测到的变量。通过计算隐变量的后验概率，可以得到对于每个样本，它属于每个隐变量取值的概率。
  3. **M步骤（Maximization Step）**: 在M步骤中，利用E步骤中计算得到的隐变量后验概率分布，对参数进行更新估计。这个步骤的目标是最大化观测数据的对数似然函数，即通过优化参数来使模型能够更好地拟合观测数据。
  4. **迭代更新**: 重复执行E步骤和M步骤，直到算法收敛或达到预设的停止条件。通常，通过计算参数的变化量来判断算法是否收敛。

EM算法的**优点**是可以用于估计存在隐变量的概率模型的参数，即使在隐变量存在的情况下，也可以对参数进行有效估计。但它也有一些**限制**，例如容易陷入局部最优解、收敛速度可能较慢等。
EM算法在许多领域都有应用，比如混合高斯模型的参数估计、隐马尔可夫模型的参数学习等。它为解决概率统计中的一类问题提供了一种有效的迭代优化方法。
事实上，隐变量估计问题也可通过梯度下降等优化算法求解，但由于求和的项数将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而**EM算法则可看作一种非梯度优化方法。**

------------------------------------------------------
# 第八章 集成学习
1. 集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统（multi-classifier system）、基于学员会的学习（committee-base learning）等。
2. 集成的方式分为“同质”（homogeneous）和“异质”（heterogenous）
![20230612193309](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612193309.png)
3. 要获得好的集成，个体学习器应**“好而不同”**，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”（diversity），即学习器间具有差异。
4. 假设基分类器的错误率相互独立，则由Hoeffding不等式可知，集成的错误率为：
![20230612193728](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612193728.png)
> 但是，错误率相互独立不可能成立，原因在于个体学习器是为解决同一问题训练出来的。准确性和多样性本就存在冲突。也就是说，如何产生并结合“好而不同”的个体学习器，恰是集成学习研究的核心。
5. 根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即个体学习器间存在强依赖关系、必须**串行**生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的**并行**化方法；前者的代表是Boosting，后者的代表是Bagging和“随机森林”(Random Forest).
6. **Boosting是一族可将弱学习器提升为强学习器的算法**。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，**再根据基学习器的表现对训练样本分布进行调整**，使得先前学习器做错的训练样本在后续受到更多关注,然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行**加权结合**。Boosting族算法最著名的代表是AdaBoost。
7. 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。
![20230612201702](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612201702.png)
8. 欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然“独立”在现实任务中无法做到,但**可以设法使基学习器尽可能具有较大的差异**。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若千个不同的子集，再从每个数据子集中训练出一个基学习器。这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。然而，为获得好的集成，我们同时还希望个体学习器不能太差。如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。为解决这个问题，我们可考虑**使用相互有交叠的采样子集**.
9. Bagging是并行式集成学习方法最著名的代表。它基于**自助采样法**。在对预测输出进行结合时，**Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法**。
10. 值得一提的是，自助采样过程还给Bagging带来了另一个优点：由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用作验证集来对泛化性能进行“包外估计”（out-of-bag estimate）
11. 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。
12. 随机森林（Random Forest，简称RF）是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中**引入了随机属性选择**。
13. 随机森林中基学习器的多样性不仅来自**样本扰动，还来自属性扰动**，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。
14. 随机森林简单、容易实现、计算开销小。值得一提的是，**随机森林的训练效率常优于Bagging**，因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，在选择划分属时要对结点的所有属性进行考察，而随机森林使用的“随机型”决策树则只需考察一个属性子集。
15. 学习器结合可能会带来的好处。
![20230612211615](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612211615.png)
16. 学习器的结合策略
    1. 平均法（averaging）：对**数值型输出** $h_i(x) \in \mathbb{R}$，最常见的结合策略是使用平均法。（平均法又分为简单平均法（simple averaging）和加权平均法（weighted averaging））
    2. 投票法：绝对多数投票法（majority voting）、相对多数投票法（plurality voting）和加权投票法（weighted voting）
> **标准的绝对多数投票法提供了“拒绝预测”选项**，这在可靠性要求较高的学习任务中是一个很好的机制。但若学习任务要求必须提供预测结果，则绝对多数投票法将退化为相对多数投票法。因此，在不允许拒绝预测的任务中，绝对多数、相对多数投票法统称为“多数投票法”

    3. 学习法：当训练数据很多时，一种更为强大的组合策略是使用“学习法”，**即通过另一个学习器来进行结合**。

17. 多样性度量（diversity measure）是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。
18. 多样性增强
    1.  数据样本扰动：数据样本扰动通常是基于采样法。**此类做法简单高效，使用最广。然而**，有一些基学习器对数据样本的扰动不敏感，例如线性学习器、支持向量机、朴素贝叶斯、k近邻学习器等，这样的基学习器称为稳定基学习器（stable base learner），对此类基学习器进行集成往往需使用输入属性扰动等其他机制。
    2.  输入属性扰动
    3.  输出表示扰动：此类做法的基本思路是对输出表示进行操纵以增强多样性
    4. <font color="#dd0000">算法参数扰动</font><br />：基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初始连接权值等，**通过随机设置不同的参数，往往可产生差别较大的个体学习器，例如“负相关法”（Negative Correlation）显式地通过正则化项来强制个体神经网络使用不同的参数**.
      - **对参数较少的算法，可通过将其学习过程中某些环节用其他类似方式代替**，从而达到扰动的目的，例如可将决策树使用的属性选择机制替换成其他的属性选择机制。
      - 值得指出的是，使用单一学习器时通常需使用交叉验证等方法来确定参数值，这事实上已使用了不同参数训练出多个学习器，只不过最终仅选择其中一个学习器进行使用，而集成学习则相当于把这些学习器都利用起来；由此也可看出，**集成学习技术的实际计算开销并不比使用单一学习器大很多**。
> 不同的多样性增强机制可同时使用。

19. 弱学习等价于强学习
20. 在集成产生之后再试图通过去除一些个体学习器来获得较小的集成，称为集成修剪（ensemble pruning）
21. 由于集成包含多个学习器，即便个体学习器有较好的可解释性，集成仍是黑箱模型。

-------------------------------------------
# 第九章 聚类
1. 无监督学习（unsupervised learning）：聚类（clustering）、密度估计（density estimation）、异常检测（anomaly-detection）
2. 聚类既能作为一个单独过程，用于找寻数据内在的分布结构，也可作为分类等其他学习任务的前驱过程。

----------------------------------------------------
## 9.1 聚类性能度量亦称聚类“有效性指标”（validity index）
1. 若明确了最终将要使用的性能度量，**则可直接将其作为聚类过程的优化目标**，从而更好地得到符合要求的聚类结果
2. 聚类结果的“簇内相似度”（intra-cluster similarity）高且“簇间相似度”（inter-cluster similarity）低.
3. 聚类性能度量大致有两类。
   1. 外部指标（external index）：将聚类结果与某个“参考模型”（reference model）进行比较
   2. 内部指标（internal index）：直接考察聚类结果而不利用任何参考模型

-------------------------------------------------------
## 9.2 距离计算
![20230612231236](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612231236.png)
![20230612231248](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612231248.png)

1. “闵可夫斯基距离”(Minkowski distance)
2. VDM (Value Diference Metric)

> 我们常将属性划分为“连续属性”(continuousattribute)和“离散属性”(categorical attribute)，前者在定义域上有无穷多个可能的取值，后者在定义域上是有限个取值。然而，在讨论距离计算时，属性上是否定义了“序”关系更为重要。
> 闵可夫斯基距离可用于有序属性，对无序属性可采用VDM

3. 将闵可夫斯基距离和VDM结合即可处理**混合属性**
4. 当样本空间不同属性的重要性不同时，可使用**“加权距离”（weighted distance）**
5. 用于相似度度量的距离未必一定药满足距离度量的所有基本性质，尤其是直递性
![20230612231416](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230612231416.png)

-----------------------------------------------------------------------------------------
3. 聚类的种类
- 原型聚类：亦称为“基于原型的聚类”（prototype-based clustering），此类算法假设聚类结构能通过一组原型刻画，**在现实聚类任务中极为常用**。
  - k均值算法
  - 学习向量量化
  - 高斯混合聚类
- 密度聚类：密度聚类亦称“基于密度的聚类”(density-based clustering)，此类算法**假设聚类结构能通过样本分布的紧密程度确定**。通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果.
- 层次聚类：层次聚类(hierarchical clustering)试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略，也可采用“自顶向下”的分拆策略.
4. 聚类也可以集成。

---------------------------------------------------------------------
# 第十章 降维和度量学习
1. 懒惰学习（lazy learning）：此类学习技术在训练阶段仅仅是把样本报错起来，训练时间开销为零，待收到测试样本后再进行处理。
2. 急切学习（eager learning）：在训练阶段就对样本进行学习处理
3. 最近邻分类器虽简单，但它的泛化错误不超过贝叶斯最优分类器的错误率的两倍！
4. 在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，**被称为“维数灾难”（curse of dimensionality）**
5. **缓解维数灾难的一个重要途径是降维（dimension reduction），亦称“维数约简”，即通过某种数学变换将原始高维属性空间转变为一个低维“子空间”（subspace）**
![20230613130159](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613130159.png)
6. **若要求原始空间中样本之间的距离在低维空间中得以保持**，如上图所示，即得到“多维缩放”（Multiple Dimensional Scaling，简称MDS）。这是一种经典的降维方法。**要求
任意两个样本在新的空间中的欧式距离等于原始空间中的距离**.
![20230613131109](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613131109.png)
7. 主成分分析（Principal Component Analysis，简称PCA），亦称“主分量分析”，是最常用的一种降维方法。
- PCA的目标是将原始数据集投影到一个新的低维空间中，**同时最大程度地保留数据的方差**。在新的低维空间中，每个维度都被称为主成分，其按照数据的方差递减的顺序排列。第一个主成分捕捉到数据中的最大方差，第二个主成分捕捉到与第一个**主成分正交（无关）的最大方差**，以此类推。
- PCA的算法步骤如下：
  - 数据预处理：对原始数据进行中心化，即将每个特征的平均值减去数据集中的平均值，以确保数据的均值为零。
  - 协方差矩阵计算：计算经过中心化的数据集的协方差矩阵。协方差矩阵反映了数据特征之间的相关性。
  - 特征值分解：对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。特征向量表示数据在主成分方向上的投影。
  - 特征值排序：按照特征值的大小降序排列特征向量，选择最大的k个特征向量对应的特征值。这些特征向量就是数据的主成分。
  - 投影到低维空间：将原始数据投影到由选取的k个主成分张成的空间中，形成降维后的数据集。
- 通过使用PCA，我们可以实现以下几个目标：
  - 数据降维：将高维数据映射到低维空间，减少数据特征的数量，从而降低计算复杂性。
  - 特征提取：通过选择最相关的主成分，保留数据中最重要的信息，并丢弃与之无关的噪声或冗余特征。
  - 数据可视化：将高维数据可视化为二维或三维图形，便于理解和解释数据。
  - PCA在数据分析、模式识别、图像处理等领域得到广泛应用，它帮助我们发现数据中的重要模式和结构，并简化复杂问题的处理。
8. 线性降维方法假设从高维空间到低维空间的函数映射是线性的，然而，在不少现实任务中，**可能需要非线性映射才能找到恰当的低维嵌入**。
9. **非线性降维的一种常用方法，是基于核技巧对线性降维方法进行“核化”（kernelized）**
10. 流形学习
    1.  等度量映射
    2.  局部线性嵌入
11. 度量学习
**在机器学习中，对高维数据进行降维的主要目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。**事实上，每个空间对应了在样本属性上定义的一个距离度量，而寻找合适的空间，实质上就是在寻找一个合适的距离度量。**那么，为何不直接尝试“学习”出一个合适的距离度量呢？这就是度量学习（metric learning）的基本动机。**
12. **主成分分析（PCA）是迄今最常用的降维方法，它有许多名字，例如线性代数中的散度矩阵奇异值分解（SVD）**、统计学中的因子分析（factor analysis）等等

--------------------------------------------------
# 第十一章 特征选择与稀疏学习
1. 对一个学习任务来说，给定属性集， 其中有些属性可能很关键、很有用，另一些属性则可能没什么用。我们将属性称为**“特征”（feature）** ，对当前学习任务有用的属性称为“相关特征”（relevant feature）、没什么用的属性称为“无关特征”（irrelevant feature）.**从给定的特征集合中选择出相关特征子集的过程，称为“特征选择”（feature selection）**
2. 特征选择是一个重要的**数据预处理**（data preprocessing）过程，这么做的原因有二：
   1. 维数灾难
   2. 去除不相关特征往往会降低学习任务的难度
3. 特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为重要信息的缺失而无法获得好的性能。
4. 特征选择中所谓的“无关特征”是指与当前学习任务无关。
5. 有一类特征称为“冗余特征”（redundant feature），它们所包含的信息能从其他特征中推演出来。更确切地说，**若某个冗余特征恰好对应了完成学习任务所需的“中间概念”，则该冗余特征是有益的**
6. 欲从初始的特征集合中选取一个包含了所有重要信息的特征子集，**若没有任何领域知识作为先验假设，那就只好遍历所有可能的子集了**，然而这在计算上却会遭遇**组合爆炸问题**.
7. 特征选择涉及两个关键环节
   1. 子集搜索（subset search）：如何根据评价结果获取下一个候选特征子集。
      1. 逐渐增加相关特征的策略称为**“前向”（forword）搜索**
      2. 逐渐减少特征的策略称为**“后向”（backward）搜索**
      3. 还可将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征（这些特征在后续轮中将确定不会被去除）、同时减少无关特征，这样的策略称为**“双向”（bidrectional）搜索**
   2. 子集评价（subset evaluation）：如何评价候选特征子集的好坏
      1. 可计算属性子集A的**信息增益**。信息增益Gain(A)越大，意味着特征子集A包含的有助于分类的信息越多。于是，对每个候选特征子集，我们可基于训练数据集D来计算其信息增益，以此作为评价准则。
      2. 更一般的，特征子集A实际上确定了对数据集D的一个划分，每个划分区域对应着A上的一个取值，而样本标记信息Y则对应着对D的**真实划分**，通过估算这两个划分的差异，就能对A进行评价。**与Y对应的划分的差异越小，则说明A越好**。
      3. **信息熵仅是判断这个差异的一种途径，其他能判断两个划分差异的机制都能用于特征子集评价**.
> 将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。

-------------------------------------------------
## 11.1 常见的特征选择方法
常见的特征选择方法大致可分为三类：过滤式（filter）、包裹式（wrapper）和嵌入式（embedding）.
1. **过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关**
2. 包裹式选择
   1. 与过滤式特征选择不考虑后续学习器不同，包裹式特征选择**直接把最终将要使用的学习器的性能作为特征子集的评价准则**.换言之，包裹式特征选择的目的**就是为给定学习器选择最有利于其性能、“量身定做”的特征子集**
   2. 由于包裹式特征选择方法直接针对给定学习器进行优化，**因此**从最终学习器性能来看，**包裹式特征选择比过滤式特征选择更好**，但另一方面，由于在特征选择过程中需多次训练学习器，因此**包裹式特征选择的计算开销通常比过滤式特征选择大得多**.
3. 嵌入式选择与$L_1$正则化
在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别；**与此不同，嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。**
![20230613184850](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613184850.png)
> $L_1$范数和$L_1$范数正则化都有助于降低过拟合风险，但前者还会带来一个额外的好处：它比后者更易于获得“稀疏”（sparse）解，即它求得的$\omega$会有很少的非零向量

![20230613185347](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613185347.png)
> 注意到$\omega$取得稀疏解意味着初始的d个特征中仅有对应着的$\omega$非零分量的特征才会出现在最终模型中，于是求解$L_1$范数正则化的结果是得到了仅采用一部分初始特征的模型；换言之，基于$L_1$正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成.

-------------------------------------------------------
8. 稀疏表示（Sparse Representation）和字典学习（Dictionary Learning）
- 稀疏表示（Sparse Representation）和字典学习（Dictionary Learning）是一种用于信号处理和机器学习中的技术，旨在通过寻找最佳的线性表示来描述数据。
- 稀疏表示是一种表示方法，它假设信号可以由一组基向量的线性组合表示，而这组基向量中只有很少一部分起作用。这意味着信号可以用很少的非零系数表示，其他系数为零。这种表示的优势在于能够提取出数据中的关键信息，并且具有较强的鲁棒性。
- 字典学习是稀疏表示的一种方法，它通过学习一个字典（也称为原子集合），其中包含一组基向量，这些基向量可以线性组合来表示数据。字典的目标是使得数据能够以稀疏的方式表示，即用尽可能少的基向量来表示尽可能多的数据。字典学习的过程通常包括以下几个步骤：
  - 初始化字典：选择一组初始的基向量作为字典。
  - 稀疏编码：使用某种优化算法，对数据进行稀疏编码，即寻找使得数据以稀疏方式表示的系数。
  - 字典更新：根据稀疏编码的结果，通过更新字典中的基向量，使其能够更好地表示数据。常见的更新方法包括迭代最小化（iterative minimization）和梯度下降等。
  - 重复步骤2和3，直到达到收敛条件或满足预定的停止准则。
- **字典学习的目标是找到一个最佳的字典，使得数据能够以最少的基向量表示，从而提取出数据的最重要特征并去除冗余信息**。它在信号处理、图像处理、模式识别等领域中具有广泛的应用，例如图像压缩、人脸识别、目标检测等。字典学习的好处在于能够适应不同类型的数据，并从数据中学习到最具代表性的基向量。

9. 压缩感知（Compressed Sensing）
- 压缩感知（Compressed Sensing）是一种基于信号处理和数学理论的新兴技术，**用于从少量的测量数据中恢复原始信号**。传统的信号采样理论要求对信号进行高密度的采样，但压缩感知则允许在采样时以远远低于传统采样要求的采样率进行采样。
- 压缩感知的核心思想是，**对于稀疏或具有某种结构的信号，可以使用远远少于信号维度的线性测量进行采样，并通过优化算法重构原始信号**。稀疏性指的是信号在某个特定域中具有较少的非零系数，或者说信号可以通过较少的基向量进行稀疏表示。
- 压缩感知的算法通常包括以下几个步骤：
  - 采样：使用低采样率的线性测量设备对信号进行采样。采样矩阵可以是随机矩阵或具有特定结构的矩阵。
  - 稀疏表示：使用适当的稀疏变换或字典，将采样到的信号表示为稀疏系数的线性组合。
  - 重构：通过优化算法，寻找最优的稀疏系数，使其与测量数据保持一致。常用的优化算法包括基于凸优化的方法，如迭代阈值算法（Iterative Thresholding）、最小二乘法（Least Squares）等。
- 压缩感知的关键概念是信号的稀疏表示和稀疏重构算法。通过采用低采样率和合适的稀疏表示方法，压缩感知能够从少量的测量数据中重构出高质量的信号。这种技术在信号处理、图像处理、通信等领域具有广泛的应用，可以实现低成本、高效率的信号采样和传输。

-----------------------------------------------------------
# 第十二章 计算学习理论
1. 泛化误差和经验误差
- 在机器学习中，泛化误差（Generalization Error）和经验误差（Empirical Error）是两个重要的概念，用于评估学习算法的性能和能力。
- **经验误差指的是在训练数据集上使用学习算法得到的误差**。它是通过将学习算法应用于已知标签的训练数据，并计算预测输出与真实标签之间的差异来计算的。经验误差衡量的是学习算法在已有数据上的拟合程度。通常，经验误差越小，说明算法在训练数据上的性能越好。
- **然而，仅仅关注经验误差是不够的，因为学习算法可能会过度拟合（Overfitting）训练数据，导致在未见过的新数据上表现不佳**。这时候就需要考虑泛化误差。
- **泛化误差是指学习算法在未见过的新数据上的误差**。它是衡量学习算法的泛化能力，即算法对未知数据的适应能力。泛化误差反映了学习算法从训练数据中学到的模式对新数据的预测性能。**理想情况下，泛化误差应该尽可能接近经验误差**。
- **过度拟合是导致泛化误差增加的常见原因**。当学习算法过度拟合训练数据时，它会过度记住训练数据的噪声和细节，而无法很好地泛化到新数据上。相反，当学习算法欠拟合（Underfitting）训练数据时，它无法捕捉到数据中的重要模式和结构，导致较高的泛化误差。
- 为了减小泛化误差，需要使用合适的模型选择、特征选择、正则化等方法。这些方法旨在平衡经验误差和泛化误差之间的关系，以获得对新数据具有良好泛化能力的模型。

---------------------------------------
## 12.1 PAC学习
- PAC（Probably Approximately Correct）学习理论是机器学习中的一个理论框架，提供了关于学习算法的性能分析和泛化保证的理论基础。
- PAC学习理论的核心思想是：**如果一个学习算法能够在有限的样本上获得高概率的正确性，且在未见过的新样本上也能取得相近的正确性，则称该算法是“可能近似正确”的**。具体来说，PAC学习理论包括以下要素：
  - 可能近似正确性：PAC学习要求学习算法以高概率（1-δ）获得近似正确的结果。其中，δ是一个很小的正数，表示学习算法在新样本上可能出现错误的概率。
  - 样本复杂性：PAC学习理论关注学习问题中的样本复杂性。样本复杂性包括样本空间的大小（样本数目）和样本分布的特征。通过研究样本复杂性，可以理解学习问题的困难程度以及学习算法所需的样本数目。
  - 假设空间：假设空间是指学习算法考虑的所有可能解的集合。PAC学习理论假设目标函数属于一个给定的假设空间，并通过分析假设空间的复杂性来研究学习问题的可解性。
  - 可学习性：PAC学习理论研究学习算法在有限样本和有限时间内是否能够从假设空间中找到一个近似正确的假设。可学习性的分析涉及到学习算法的效率和泛化性能的折衷。
PAC学习理论为机器学习提供了一种理论保证，即在一定条件下，学习算法可以通过有限样本来获得近似正确的结果，并在未见过的新样本上具有较好的泛化能力。这一理论框架对于设计、分析和改进学习算法具有重要的指导作用，促进了机器学习的理论研究和实践发展。

-----------------------------------------------------------
- PAC辨识：PAC辨识是指在PAC学习理论中，通过有限的样本和有限的时间，确定出一个近似正确的目标函数的过程。PAC辨识问题的关键是找到一个学习算法，能够以高概率获得近似正确的结果。
- PAC可学习：PAC可学习性是指在PAC学习理论中，给定一个假设空间和一个概念类，判断是否存在一个学习算法可以以高概率获得近似正确的结果。PAC可学习性的分析涉及到学习问题的可解性和假设空间的复杂性。
- PAC学习算法：PAC学习算法是指在PAC学习理论中，根据有限样本和有限时间的限制，通过学习过程找到一个近似正确的假设。PAC学习算法的设计和分析是研究学习问题的可解性和泛化能力的关键。
- 样本复杂度：样本复杂度是指在PAC学习理论中，描述学习问题所需的样本数目。样本复杂度分析研究了学习算法所需的样本数目与学习问题的性质、假设空间的复杂性以及算法的性能之间的关系。
这些概念在PAC学习理论中相互关联，共同构成了理论框架的核心。PAC学习理论通过研究这些概念的关系，旨在提供关于学习算法性能、泛化保证和学习问题可解性的理论保证。这为机器学习的设计、分析和实践提供了指导，并推动了机器学习理论的发展和应用。

----------------------------------------------------------------
## 12.2 有限假设空间
**有限假设空间（Finite Hypothesis Space）是指在学习问题中，用于表示可能解的假设的集合是有限的**。假设空间可以包含各种可能的函数、模型或参数组合，用于描述学习算法可以选择的所有可能解。**有限假设空间假设了学习问题的解空间是可枚举且有限的**。
### 12.2.1 可分情形（Separable Case）
在可分情形中，**假设空间中存在一个假设能够完美地将训练样本进行分类**。也就是说，存在一个假设能够将正例和负例完全分开。这种情况下，学习算法可以找到一个完美的分类器，能够在训练数据上取得零经验误差，并具备在新样本上取得零泛化误差的潜力。
### 12.2.2 不可分情形（Non-Separable Case）
在不可分情形中，**假设空间中的任何假设都无法完美地将训练样本进行分类**，即不存在一个超平面或决策边界能够完美分隔正例和负例。在这种情况下，学习算法无法达到零经验误差，并且无法在新样本上取得零泛化误差。
- "不可知PAC可学习"（AgnoPAC Learnable）
不可知PAC可学习的情况与可分情形和不可分情形有一定关系。在可分情形下，我们可以确定存在一个学习算法能够完美地将训练数据进行分类，因此该问题是PAC可学习的。**而在不可分情形下，我们无法确定是否存在一个学习算法能够达到PAC可学习的要求，即我们无法确定问题的可解性。因此，不可知PAC可学习常常与不可分情形相关联**。

不可知PAC可学习的情况表明对于某些学习问题，**我们无法确定是否存在一个学习算法能够在有限样本和有限时间内获得近似正确的解**。这种情况下，我们需要进一步研究学习问题的性质，探索其他的算法或技术来处理这些问题。不可知PAC可学习的情况属于一种特殊的情况，需要更深入的研究和分析才能得出结论。

-----------------------------------------------------------------
## 12.3 VC维（Vapnik-Chervonenkis Dimension）
**现实学习任务所面临的通常是无线假设空间**，例如实数域中的所有区间、$\mathbb{R}^d$空间中的所有线性超平面。欲对此种情形的可学习性进行研究，**需度量假设空间的复杂度。最常见的办法是考虑假设空间的“VC维”（Vapnik-Chervonenkis Dimension）**
- VC维（Vapnik-Chervonenkis Dimension）是统计学习理论中的一个重要概念，**用于衡量一个假设空间的表达能力或复杂性**。
- VC维是由Vladimir Vapnik和Alexey Chervonenkis提出的，**它描述了一个假设空间能够在样本上自由分布并完全正确分类的最大样本数量**。换句话说，VC维是指一个假设空间可以灵活地适应任何可能的样本配置，并能够对这些样本进行完美的分类的能力。
- **具体而言，给定一个假设空间H，VC维被定义为该假设空间可以破坏任意大小的样本集合的最大值。也就是说，VC维是假设空间中能够存在的最大破坏样本数量的大小。**
- VC维的重要性在于它与学习算法的泛化性能相关。**根据VC维理论，如果一个假设空间的VC维较小，则学习算法在该假设空间上具有较好的泛化能力，即能够从有限的样本中推广到未见过的新样本。相反，如果一个假设空间的VC维较大，则可能导致学习算法出现过拟合的问题，即在训练数据上表现良好，但在新样本上的性能较差**。
- 在实际应用中，研究者和从业者利用VC维理论来评估和选择合适的假设空间和模型复杂度，以平衡模型的表达能力和泛化能力。此外，VC维还与学习算法的样本复杂度和收敛速度等方面的分析有关，对于理解学习问题的困难程度和算法性能具有重要意义。

-------------------------------------------------------------------
## 12.4 Rademacher复杂度（Rademacher Complexity）
**基于VC维的泛化误差界是分布无关、数据独立的，也就是说，对任何数据分布都成立。**这使得基于VC维的可学习性分析结果具有一定的“普适性”；但从另一方面来说，由于没有考虑数据自身，基于VC维得到的泛化误差界通常比较“松”，对那些与学习问题的典型情况相差甚远的较“坏”分布来说尤其如此。
**Rademacher 复杂度（Rademacher complexity）是另一种刻画假设空间复杂度的途径，与VC维不同的是，它在一定程度上考虑了数据分布**

- Rademacher复杂度（Rademacher Complexity）是统计学习理论中用于度量假设空间复杂度的一种方法。它由Rademacher符号引入，**可以用于估计一个假设空间的泛化能力。**
- **Rademacher复杂度通过引入随机变量的符号来评估假设空间在随机样本上的表现。具体而言，给定一个假设空间H和一个样本集合，Rademacher复杂度衡量了该假设空间中所有可能的函数对样本的拟合程度。**
- 具体计算Rademacher复杂度的过程如下：
  1. 从一个取值在[-1, 1]之间的均匀分布中抽取与样本集合中样本数量相同的随机变量，称为Rademacher符号。
  2. 对于假设空间H中的每个函数，计算该函数在样本集合上的平均值，并与Rademacher符号相乘。
  3. 对所有函数的平均值取最大值，得到Rademacher复杂度。
- Rademacher复杂度可以理解为假设空间的平均适应度，或者说是假设空间中函数对随机样本的平均适应度。**较小的Rademacher复杂度表示假设空间具有较强的泛化能力，能够从有限的样本中推广到未见过的新样本。**
- Rademacher复杂度与VC维密切相关，它们在分析假设空间的复杂性和泛化性能方面具有一定的相似性。**然而，它们的计算方式和理论基础有所不同。Rademacher复杂度更注重样本集合与假设空间之间的匹配度，而VC维更关注假设空间中样本集合的分布模式和破坏能力。**
- Rademacher复杂度为我们提供了一种量化假设空间复杂度的工具，用于评估学习算法的泛化能力和性能。它在理论分析和算法设计中具有重要作用。

------------------------------------------------------------
## 12.5 稳定性（stability）
**无论是基于VC维还是Rademacher复杂度来推导泛化误差界，所得到的结果均与具体学习算法无关，对所有学习算法都适用**。这使得人们能够脱离具体学习算法的设计来考虑学习问题本身的性质，但在另一方面，**若希望获得与算法有关的分析结果，则需另辟蹊径。稳定性(stability) 分析**是这方面一个值得关注的方向.
顾名思义，**算法的“稳定性”考察的是算法在输入发生变化时，输出是否会随之发生较大的变化.**

- 稳定性分析是一种用于评估学习算法的性质的方法，**关注算法在输入发生变化时输出的稳定性程度**。稳定性分析主要研究学习算法的输入域上的小扰动对输出的影响，并通过量化输出的变化程度来度量算法的稳定性。
- 在机器学习中，算法的稳定性分析通常涉及以下两个方面：
  1. **输入扰动：稳定性分析考虑在输入数据发生小幅度扰动或变化时，算法的输出是否会产生显著的变化**。这些输入扰动可以是针对训练数据的微小变化，或者是对模型输入的微小变化。**稳定性分析可以帮助我们了解算法对数据中的噪声或不确定性的敏感程度。**
  2. **算法变化：稳定性分析还可以研究在算法本身发生微小变化时，其输出的变化情况**。这包括对算法参数、模型选择、优化方法等的微小调整或变化。**通过稳定性分析，我们可以了解算法的鲁棒性和对变化的适应能力。**
- 稳定性分析在理论研究和实际应用中都具有重要意义：
  1. 理论分析：稳定性分析为我们提供了一种理解学习算法行为的方式，可以帮助我们研究算法的一致性、泛化能力以及与其他算法的比较等方面。通过稳定性分析，我们可以脱离具体的学习算法细节，探究学习问题本身的性质和算法的一般性质。
  2. 算法设计：稳定性分析可以指导学习算法的设计和改进。通过考虑算法的稳定性，我们可以选择更稳定的算法或改进算法以提高其稳定性。稳定性的提高可以降低算法对输入的噪声和变化的敏感性，增强算法的鲁棒性。
- 总的来说，稳定性分析在研究学习算法的性质、评估算法的鲁棒性和推导算法性能上具有重要作用。它为我们提供了一种从另一个角度理解和分析学习算法的方法，有助于深入研究学习问题并设计更鲁棒、稳定的算法。


-------------------------------------------------------
# 第十三章 半监督学习
1. 主动学习（Active Learning）
  - 主动学习（Active Learning）是一种机器学习的方法，旨在提高训练模型的效率和准确性。在传统的机器学习中，通常需要大量的标记数据来训练模型，然后使用该模型进行预测。但是，标记数据的收集和准备往往是耗时和昂贵的。**主动学习的目标是通过选择最有价值的样本来减少标记数据的需求。**
  - 主动学习的**基本思想是，在训练模型的过程中，不仅仅是被动地使用已有的标记数据，而是主动地选择一些未标记的样本**，然后请人类专家或模型本身来标记这些样本。**选择哪些样本进行标记的过程是主动学习的关键。**
  - **主动学习通常使用一种叫做"不确定性采样"（Uncertainty Sampling）的方法来选择要标记的样本**。这种方法**会在模型对未标记样本进行预测时，关注那些模型预测结果最为不确定的样本**。例如，在二分类问题中，一个样本可能距离分类决策边界很近，模型很难确定其分类，那么这个样本就是一个不确定的样本，可以选择标记它来改善模型的预测能力。
  - 另一种常用的主动学习方法是**基于模型不确定性的抽样方法**，例如使用不确定性估计（Uncertainty Estimation）的方法，如基于不确定性的置信度或熵等指标来选择样本。**这些方法通常利用模型在样本空间的不确定性来衡量样本的价值，以便在训练过程中选择最具信息量的样本。**
  - **通过选择最有价值的样本进行标记，主动学习可以在相对较少的标记数据的情况下构建高性能的模型**。这对于训练数据有限或者标记数据成本较高的任务尤为重要。通过主动学习，可以最大程度地利用有限的标记数据，从而提高模型的泛化能力和预测性能。

2. 事实上，**没有专家干涉，也可利用无标记样本数据的信息。**
若能观察到图中的未标记样本，则将很有把握地判别为正例。
![20230613213915](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613213915.png)
3. 让学习器不依赖外界交互，自动地利用未标记样本来提升学习性能，就是半监督学习（semi-supervised learning）
4. **要利用未标记样本，必然要做一些将未标记样本所揭示的数据分布信息与类别标记相联系的假设。**
   1. 聚类假设（cluster assumption）
   2. 流形假设（manifold assumption）
> 事实上，无论聚类假设还是流形假设，其本质都是“相似的样本拥有相似的输出”这个基本假设
5. **半监督学习可进一步划分为纯（pure）半监督学习和直推学习（transductive learning），前者假定训练数据中的未标记样本并非待预测的数据，而后者则假定学习过程中所考虑的未标记样本恰是待预测数据，学习的目的就是在这些未标记样本上获得最优泛化性能。**
一图胜千言：
![20230613214852](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613214852.png)
6. 生成式方法（Generative Methods）
- 生成式方法（Generative Methods）是直接基于生成式模型的方法。**此类方法假设所有数据（无论是否有标记）都是由同一个潜在的模型“生成”的。**这个假设使得我们能通过潜在模型的参数将未标记数据与学习目标联系起来，而未标记数据的标记则可看作模型的确实参数。
- **此类方法的区别主要在于生成式模型的假设，不同的模型假设将产生不同的方法。**
- 遗憾的是，**在现实任务中往往很难事先做出准确的模型假设，除非拥有充分可靠的领域知识。**
- 生成式方法常用的模型包括：
  1. 高斯混合模型（Gaussian Mixture Model，GMM）：GMM 是一种**概率模型，假设数据由多个高斯分布组合而成**。通过学习高斯分布的参数，GMM 可以生成新的数据样本。
  2. 隐马尔可夫模型（Hidden Markov Model，HMM）：HMM 是一种**时序模型，用于建模具有隐含状态的序列数据**。HMM 假设数据的生成过程是一个马尔可夫过程，通过学习转移矩阵和观测概率，HMM 可以生成与原始序列类似的新序列。
  3. 生成对抗网络（Generative Adversarial Network，GAN）：**GAN 是一种基于博弈论的生成模型，由生成器（Generator）和判别器（Discriminator）组成**。生成器试图生成逼真的数据样本，而判别器则试图区分生成的样本和真实样本。通过对抗训练，生成器逐渐学习生成高质量的数据样本。
  4. 变分自编码器（Variational Autoencoder，VAE）：**VAE 是一种基于神经网络的生成模型，通过学习数据样本的潜在变量分布，实现数据的生成和重构**。VAE 通过编码器将数据映射到潜在空间，再通过解码器将潜在变量映射回数据空间，从而生成新的数据样本。

7. 半监督支持向量机(Semi-Supervised Support Vector Machine, 简称S3VM)是支持向量机在半监督学习上的推广。**在不考虑未标记样本时，支持向量机试图找到最大间隔划分超平面，而在考虑未标记样本后，S3VM试图找到能将两类有标记样本分开，且穿过数据低密度区域的划分超平面**.
8. 半监督SVM的**基本思想是，通过在支持向量机的优化目标函数中引入无标签数据的约束，利用无标签数据的分布信息来提供额外的学习信号**。这样，模型可以通过利用无标签数据的分布特征来更好地学习分类边界，从而提高分类器的性能。
半监督SVM的训练过程包括以下步骤：
  1. 初始训练：使用有标签数据训练一个标准的支持向量机分类器。
  2. 标签传播：**使用该分类器对无标签数据进行预测，将预测结果作为无标签数据的伪标签。**
  3. 重新训练：将有标签数据和带有伪标签的无标签数据合并，重新训练支持向量机分类器。
  4. 重复迭代：重复执行步骤2和步骤3，直到达到一定的迭代次数或收敛条件。
通过不断迭代的过程，半监督SVM可以逐渐利用无标签数据的信息进行学习，从而提高分类器的泛化能力。
半监督SVM的优点在于可以利用未标记数据来提供额外的信息，从而扩大训练数据集，改善模型性能。它尤其适用于标记数据稀缺或者标记过程代价较高的情况。然而，**半监督SVM也面临一些挑战，如如何准确传播标签、无标签数据中的噪声以及类别不平衡等问题**。因此，在使用半监督SVM时需要谨慎处理这些问题，以获得更好的结果。

9. 图半监督学习（Graph Semi-Supervised Learning）
- **给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图中一个结点，若两个样本之间的相似度很高(或相关性很强)，则对应的结点之间存在一条边, 边的“强度”(strength)正比于样本之间的相似度(或相关性)**。我们可将有标记样本所对应的结点想象为染过色，而未标记样本所对应的结点尚未染色。于是，半监督学习就对应于“颜色”在图上扩散或传播的过程。**由于一个图对应了一个矩阵，这就使得我们能基于矩阵运算来进行半监督学习算法的推导与分析.**
- 图半监督学习（Graph Semi-Supervised Learning）是一种机器学习方法，旨在利用图结构数据中的标记和未标记节点信息来进行分类或回归任务。**与传统的半监督学习方法相比，图半监督学习更适用于处理具有图结构的数据，例如社交网络、推荐系统、生物网络等。**
- 图半监督学习的**基本思想是将节点之间的连接关系转化为图结构，利用图中节点的拓扑结构和特征信息来推断未标记节点的标签**。以下是图半监督学习的关键要素和步骤：
  1. 图构建：将数据集中的节点和它们之间的连接关系表示为图结构。连接关系可以是节点之间的边、邻接矩阵或者相似性矩阵。
  2. 节点特征提取：为每个节点提取特征表示，这些特征可以是节点的属性、结构信息或其他相关信息。常用的特征提取方法包括节点属性传播、图卷积网络（Graph Convolutional Networks）等。
  3. 标记传播：**利用已标记节点的标签信息，通过图结构进行标签传播，将已标记节点的标签信息传递给未标记节点**。传播的方式可以是基于图的扩散算法，如拉普拉斯传播（Laplacian Propagation）或基于图卷积网络的方法。
  4. 学习模型：基于已标记和未标记节点的特征和标签信息，训练一个分类器或回归模型来进行预测。常用的模型包括支持向量机、随机森林、神经网络等。
- 图半监督学习的优点在于它能够利用图结构中的节点连接关系和特征信息来推断未标记节点的标签，从而提供更准确的预测结果。它对于处理具有图结构的数据和标记数据不充分的情况下特别有用。然而，**图半监督学习也面临一些挑战，如图噪声、图稀疏性、标签传播的准确性等问题。**
- 图半监督学习方法在概念上相当清晰，且易于通过对涉矩阵运算的分析来探索算法性质。但此类算法的缺陷也相当明显：
  - 存储开销
  - 由于构图过程中仅能考虑训练样本集，**难以判知新样本在图中的位置。**

10. 基于分歧的方法（Divergence-Based Methods）
与生成式方法、半监督SVM、图半监督学习等基于单学习器利用未标记数据不同，**基于分歧的方法（Divergence-Based Methods）使用多学习器，而学习器之间的“分歧”（disagreement）对未标记数据的利用至关重要**
11. 聚类是一种典型的无监督学习任务，然而在现实聚类任务中我们往往能获得一些额外的监督信息。于是可通过**半监督聚类（Semi-Supervised Clustering）**来利用监督信息以获得更好的聚类效果。
12. 聚类任务中获得的监督信息大致有两种，**第一种类型是“必连”（must-link）与“勿连”（cannot-link）约束，前者是指样本必属于同一个簇，后者是指样本必不属于同一个簇**
13. 阅读材料
![20230613225704](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230613225704.png)
> 许多集成学习研究者认为：只要能使用多个学习器即可将弱学习器性能提升到极高，无须使用未标记样本；许多半监督学习研究者认为：只要能使用未标记样本即可将弱学习器性能提升到极高，无须使用多学习器。但这两种看法都有其局限.
14. 半监督学习**不仅仅可应用于半监督分类和聚类，也可用于半监督回归、降维等方面**
15. **确保半监督学习的泛化性能至少不差于仅利用有标记样本仍是一个未解决的问题**


---------------------------------------------------------
# 第十四章 概率图模型
## 14.1 先修知识
- 概率图模型（Probabilistic Graphical Model，PGM）
  - 概率图模型（Probabilistic Graphical Model，PGM）是一种用于建模概率关系的图结构模型。**它能够表示变量之间的依赖关系**，并且使用概率分布来描述这些依赖关系。
  - 概率图模型主要有两种类型：**贝叶斯网络（Bayesian Network）和马尔可夫随机场（Markov Random Field）。**
    1. **贝叶斯网络（Bayesian Network，也称为有向图模型）：贝叶斯网络使用有向无环图（DAG）表示变量之间的依赖关系**。节点表示随机变量，边表示变量之间的条件依赖关系。每个节点都与一个条件概率表相关联，该表描述了给定其父节点的取值时，该节点的取值的概率分布。**贝叶斯网络可以用来进行因果推断和概率推理**，常用于处理不确定性推理和决策问题。
    2. **马尔可夫随机场（Markov Random Field，也称为无向图模型）：马尔可夫随机场使用无向图表示变量之间的依赖关系**。图中的节点表示随机变量，边表示变量之间的相关性。每个节点与一个势函数（或能量函数）相关联，该函数度量了该节点与其邻居节点的关系。马尔可夫随机场主要用于建模联合概率分布，可以用于图像分割、图像处理、计算机视觉等领域。
  概率图模型具有许多优点，包括清晰的可视化表示、模型的简洁性和模块化性质。它们提供了一种灵活的框架，用于对复杂的概率关系建模和推理。概率图模型在机器学习、人工智能、统计学和模式识别等领域中被广泛应用，包括决策网络、隐马尔可夫模型和条件随机场等。

- 隐马尔可夫模型（Hidden Markov Model，HMM）
  - 隐马尔可夫模型（Hidden Markov Model，HMM）是一种用于**建模序列数据**的统计模型。它是马尔可夫模型的扩展，**适用于存在状态（观察不到）和观察（可见）之间的关系的问题。**
  - **HMM由两个基本要素组成：状态和观察**。状态是隐藏的，表示系统内部的一种特定状态；观察是可见的，表示我们能够观测到的数据。在HMM中，系统在不同的状态之间转换，并且每个状态生成特定的观察。
  - **HMM由以下几个部分组成**：
    1. 状态集合（State set）：包含系统可能的状态。记为S={$s_1, s_2, ..., s_n$}。
    2. 观察集合（Observation set）：包含可以观察到的数据。记为O={$o_1, o_2, ..., o_m$}。
    3. 状态转移概率矩阵（Transition probability matrix）：表示状态之间的转换概率。记为A={$a_{ij}$}，其中{$a_{ij}$}表示从状态$s_i$转移到状态$s_j$的概率。
    4. 观察概率矩阵（Observation probability matrix）：表示在给定状态下生成观察的概率。记为B={$b_{ij}$}，其中B={$b_{ij}$}表示在状态$s_i$生成观察$o_j$的概率。
    5. 初始状态概率向量（Initial state probability vector）：表示系统初始状态的概率分布。记为π={$π_i$}，其中$π_i$表示系统初始状态为$s_i$的概率。
  - **HMM有三个基本问题**：
    1. 评估问题（Evaluation problem）：给定模型λ=(A, B, π)和观察序列O，计算给定观察序列的概率P(O|λ)。
    2. 解码问题（Decoding problem）：给定模型λ=(A, B, π)和观察序列O，找到最可能的对应状态序列。
    3. 学习问题（Learning problem）：给定观察序列O，估计模型λ=(A, B, π)的参数。
  - HMM的应用非常广泛，特别适用于语音识别、自然语言处理、生物信息学和时间序列分析等领域。通过对状态和观察之间的关系进行建模，HMM可以用来预测未来的状态，进行序列标注和分类，以及进行序列生成等任务。

- 马尔可夫随机场（Markov Random Field，MRF）
  - **马尔可夫随机场（Markov Random Field，MRF）是一种用于建模联合概率分布的无向图模型**。它描述了一组变量之间的相关性，并且基于马尔可夫性质，即给定一个变量集合的条件下，**每个变量只依赖于其邻居节点的取值**。
  - MRF由一个无向图表示，图中的节点表示随机变量，边表示变量之间的相关性。**与有向图模型（如贝叶斯网络）不同，MRF没有明确的因果关系，变量之间的依赖关系是对称的**。
  - MRF中的变量可以分为两类：
    1. 可见变量（Observable Variables）：表示我们能够直接观察到的变量。
    2. 隐藏变量（Hidden Variables）：表示我们无法直接观察到的变量。
  - **MRF使用势函数（Potential Function）来度量变量之间的相关性**。势函数定义在**图中的团（Clique）上，一个团是指图中完全连接的一组节点**。势函数通常表示为指数函数的形式，其中指数的值与团的配置相关。
  - MRF的联合概率分布可以通过对势函数的归一化常数进行标准化来计算。给定MRF的势函数和变量的取值，可以计算出整个网络的联合概率分布。
  - MRF可以用于许多任务，包括图像处理、计算机视觉、模式识别和统计推断等。例如，在图像处理中，MRF被广泛用于图像分割、图像去噪和图像恢复等任务，其中变量通常表示图像的像素或区域。
  - **对于MRF，推断问题是一个重要的任务**，包括计算变量的边缘概率、最大后验概率估计和找到具有最高概率的变量配置。**常用的推断算法包括信念传播（Belief Propagation）、Gibbs采样和变分推断等**。
  - 总而言之，马尔可夫随机场是一种强大的概率图模型，能够用于建模变量之间的相关性。它具有广泛的应用领域，并在许多机器学习和人工智能任务中发挥重要作用。

---------------------------------------------------
1. 概率模型（probabilistic model）提供了一种描述框架，将学习任务归结于计算变量的概率分布。**在概率模型中，利用已知变量推测未知变量的分布称为“推断”（inference）**，其核心是如何基于可观测变量推测出未知变量的**条件分布**。**直接利用概率求和规则消去变量R显然不可行，因为即便每个变量仅有两种取值的简单问题，其复杂度已至少是$O(2^{|Y|+|R|})$.
2. 概率图模型（probabilistic graphical model）是一类用图来表达变量相关关系的概率模型。它以图为表示工具，最常见的是用一个结点表示一个或一组随机变量，节点之间的边表示变量间的概率相关关系，即“变量关系图”
3. 根据边的性质不同，概率图模型可大致分为两类：
   1. 使用**有向无环图**表示变量间的**依赖关系**，称为**有向图模型或贝叶斯网（Bayesian netword）**
   2. 使用**无向图**表示变量间的**相关关系**，称为**无向图模型或马尔可夫网（Markov network）**
4. **隐马尔可夫模型（Hidden Markov Model，简称HMM）是结构最简单的动态贝叶斯网（dynamic Bayesian network），这是一种著名的有向图模型，主要用于时序数据建模**
5. **隐马尔可夫用到了所谓的“马尔科夫链”（Markov chain），即：系统下一时刻的状态仅由当前状态决定，不依赖于以往的任何状态**
6. 隐马尔可夫模型的三个基本问题：
![20230614100417](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614100417.png)
> 值得庆幸的是，基于条件独立性，隐马尔可夫模型的这三个问题均能被高效求解

7. 马尔可夫随机场（Markov Random Field，MRF）是典型的马尔可夫网，这是一种著名的**无向图模型**。图中每个结点表示一个或一组变量，节点之间的边表示两个变量之间的**依赖关系**。马尔可夫随机场有一组**势函数（potential functions），亦称“因子”（factor），这是定义在变量子集上的非负实函数，主要用于定义概分布函数**.对于图中结点的一个子集，**若其中任意两结点间都有边连接，则称该结点子集为一个“团”（clique）**.若在一个团中加入另外任何一个结点都不再形成团，则称该团为“极大团”（maximal clique）；**换言之，极大团就是不能被其他团所包含的团**。

TODO


-----------------------------------------------------------
# 第十五章 规则学习
1. “规则学习”（rule learning）是从训练数据中学习出一组能用于对未见示例进行判别的规则。**规则学习中的“规则”是狭义的，事实上约定俗成地省略了“逻辑”二字**。形式化地看，一条规则形如：
![20230614221645](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614221645.png)
> 其中逻辑蕴含符号“$\leftarrow$”右边部分称为“规则体”（body），表示该条规则的前提，左边部分称为“规则头”（head），表示该条规则的结果。

2. 规则体是由逻辑文字（literal）$f_k$组成的合取式（conjunction），其中合取符号“∧”用来表示“并且”。每个文字$f_k$都是对示例属性进行检验的布尔表达式。
3. 规则学习有更好的可解释性
4. **数理逻辑具有极强的表达能力**。因此，规则学习能更自然地在学习过程中引入领域知识。
5. 符合某条规则地样本称为被该规则“覆盖”（cover）。**需要注意的是，被正例规则覆盖的样本是正例，但没被正例规则覆盖的未必不是正例；只有被反例规则覆盖的样本才是反例**。
6. **显然，规则集合中的每条规则都可看作一个子模型，规则集合是这些子模型的一个集成**。
7. **当同一个示例被判别结果不同的多条规则覆盖时，称发生了“冲突”（conflict），解决冲突的办法称为“冲突消解”（conflict resolution）**。常用的冲突消解策略有投票法、排序法、元规则法等。
8. **从训练集学得的规则集合也许不能覆盖所有可能的未见示例，这种情况在属性数目很多时常出现。因此，规则学习算法通常会设置一条“默认规则”（default rule），由它来处理规则集合未覆盖的样本**。
9. 从形式语言表达能力而言，规则可分为两类：**“命题规则”（propositional rule）和“一阶规则”（first-order rule）**.
- 显然，**一阶规则能表达复杂的关系**，因此也被称为“关系型规则”（relational rule）
- 显然，从形式语言系统的角度来看，**命题规则是一阶规则的特例，因此一阶规则的学习比命题规则要复杂得多**
10. **规则学习的目标是产生一个能覆盖尽可能多的样例的规则集。最直接的做法是“序贯覆盖”(sequentialcovering)，即逐条归纳**：在训练集上每学到一条规则，就将该规则覆盖的训练样例去除，然后以剩下的训练样例组成训练集重复上述过程。由于每次只处理一部分数据，因此**也被称为“分治”(separate- and conquer)策略**.
11. **序贯覆盖（Sequential Covering）**是一种基于规则学习的机器学习方法，用于从数据中提取一组规则来描述数据之间的关系。它的目标是**生成一组简单而准确的规则，每个规则都能覆盖尽可能多的训练实例**。
- 序贯覆盖通常用于**监督学习**任务，其中训练数据包含输入特征和相应的输出标签。它逐步生成规则，每次生成一条规则并移除该规则覆盖的实例，然后再基于更新后的数据继续生成下一条规则。**这种逐步生成的过程可以确保每个规则都专注于不同的实例子集**。
- 下面是序贯覆盖的基本步骤：
  1. 初始化：准备训练数据集和一个初始规则集。
  2. 选择规则：从剩余的未覆盖实例中选择一个规则进行生成。**通常采用启发式方法**，如基于信息增益或基尼指数的特征选择来确定生成规则的顺序。
  3. 生成规则：针对选择的规则，根据数据的特征和标签生成规则的条件和结论。生成的规则应该足够简单、具有高准确性，并且能够覆盖尽可能多的实例。
  4. 移除实例：将被生成的规则覆盖的实例从数据集中移除，以确保下一个规则能够关注到其他未覆盖的实例。
  5. 终止条件：**检查终止条件，例如达到指定的规则数量、无法生成更多规则或覆盖率达到一定阈值等**。
  6. 重复生成：如果终止条件未满足，返回步骤2，选择下一个规则进行生成。
- 序贯覆盖的优点之一是生成的规则简单且易于解释，因为每个规则都是基于部分实例生成的，并且可以根据规则的覆盖情况来评估其重要性。然而，**序贯覆盖也有一些挑战，例如处理大规模数据集时的效率问题以及规则之间的重叠和冲突**。在实际应用中，可以使用一些技术来改进序贯覆盖算法，例如**剪枝、规则合并和后剪枝等**。

12. **一阶规则学习（First-Order Rule Learning）**是机器学习领域中的一种方法，用于从数据中学习一阶逻辑规则。与传统的规则学习方法相比，一阶规则学习能够处理更丰富和复杂的关系，包括对象之间的关联和属性之间的约束。
**一阶逻辑是一种用于描述关系和约束的形式化语言，它包含谓词（描述对象的性质或关系）、变量（代表对象）、常量（具体的对象）和逻辑连接词（如AND、OR、NOT等）。一阶规则学习的目标是通过学习从输入特征到输出标签的一阶逻辑规则来进行分类、预测或推理**。
以下是一阶规则学习的一般步骤：
  1. 数据准备：收集和准备用于学习的数据集，包括一组特征和相应的标签。
  2. 特征工程：根据任务和数据的特点，对输入特征进行处理和选择，以提取有意义的一阶逻辑约束。
  3. 规则生成：使用一阶规则学习算法从数据集中生成一阶逻辑规则。这通常包括确定规则的结构、选择谓词和变量，并确定规则的条件和结论。
  4. 规则评估：评估生成的规则对于新数据的预测能力。可以使用交叉验证或保留一部分数据进行评估。
  5. 规则应用：将生成的规则应用于新的未知实例，进行分类、预测或推理。
**一阶规则学习的优点之一是能够处理复杂的关系和约束，因为一阶逻辑具有丰富的表达能力。它可以应用于多个领域，包括自然语言处理、知识图谱构建、关系抽取等任务。然而，一阶规则学习也面临一些挑战，如处理大规模数据集的计算复杂性和过拟合问题**。近年来，一阶规则学习领域涌现出一些高效的算法和技术，如基于逻辑编程的方法、约束学习和归纳逻辑编程等，以改善一阶规则学习的效果和效率。

13. **归纳逻辑程序设计（Inductive Logic Programming，简称ILP）**是一种机器学习方法，**结合了一阶逻辑（First-Order Logic）和逻辑程序设计的思想，用于从数据中归纳出一组逻辑程序规则。ILP的目标是通过学习逻辑规则来解决基于背景知识的逻辑程序归纳问题，这些规则可以直接在逻辑程序设计语言（如PROLOG）中使用**。
- 在ILP中，背景知识以逻辑程序的形式给出，它包括一组事实、规则和约束，描述了问题领域的先验知识。ILP的任务是根据给定的背景知识和训练数据集，学习出一组逻辑规则，使得这些规则能够解释已知的数据，并具有一定的泛化能力。
- ILP的基本思想是通过在逻辑程序中插入变量和函数，使得规则可以通过归纳推理从例子中学习出来。ILP算法通常包括以下步骤：
  1. 定义背景知识：将问题的背景知识表示为逻辑程序，包括事实、规则和约束。
  2. 数据准备：收集和准备用于学习的训练数据集，其中每个示例由输入和输出标签组成。
  3. 候选规则生成：基于背景知识，生成候选的逻辑规则，这些规则包含变量和函数，以便进行归纳推理。
  4. 归纳推理：使用归纳推理算法，将候选规则与训练数据集进行匹配和推理，以找到满足数据的规则。
  5. 规则评估和修剪：评估生成的规则对于训练数据和背景知识的拟合程度，并进行修剪或组合，以提高规则的泛化能力。
  6. 输出规则：输出学得的逻辑规则，这些规则可以直接在逻辑程序设计语言中使用，例如PROLOG。
- 归纳逻辑程序设计具有强大的表达能力，能够处理复杂的关系和约束。它在领域知识比较充分、问题具有明确的逻辑结构的任务中表现出色，例如自然语言处理、关系抽取、知识图谱构建等。然而，由于ILP的计算复杂性较高，处理大规模数据和复杂问题时可能面临挑战。因此，研究者们一直在探索和改进ILP算法，以提高其效率和可扩展性。


--------------------------------------------------------
# 第十六章 强化学习
1. 强化学习（reinforcement learning）
  - 强化学习（Reinforcement Learning，RL）是机器学习的一个分支，**用于解决智能体（Agent）在与环境进行交互的过程中，通过试错和学习来获得最优行为策略的问题**。强化学习是一种通过奖励信号来引导学习的方法，智能体通过与环境的交互，通过尝试不同的行动并接收环境的反馈来学习如何在给定的环境中做出最佳的决策。
  - 强化学习涉及以下几个核心元素：
    1. 环境（Environment）：强化学习中智能体所处的外部环境，它包含智能体**可以观察到的状态和智能体可以采取的动作**。
    2. 智能体（Agent）：强化学习的学习主体，通过观察环境的状态，选择合适的动作以最大化累积奖励。
    3. 状态（State）：环境的观察值，用于描述环境的特定情况。
    4. 动作（Action）：智能体在给定状态下可以执行的操作或决策。
    5. 奖励（Reward）：环境在智能体执行动作后**提供的反馈信号，用于指导智能体的学习过程。奖励可以是正向的（鼓励行为）或负向的（惩罚行为）**。
  - 强化学习的目标是通过学习最优的策略，使智能体在环境中**获得最大的累积奖励。学习过程通常基于价值函数（Value Function）或策略（Policy）的优化**。价值函数用于评估在给定状态下采取特定动作的**长期回报，而策略定义了智能体在给定状态下应该采取的动作**。
  - 强化学习算法可以基于**模型（Model-Based）或无模型（Model-Free）进行分类**。模型基础的方法通过学习环境的模型（包括状态转移和奖励函数）来做出决策。无模型的方法则直接从与环境的交互中学习，通过尝试和错误来更新价值函数或策略。
  - 强化学习在许多领域有广泛应用，包括机器人控制、自动驾驶、游戏玩法、资源管理和金融交易等。著名的强化学习算法包括Q-learning、SARSA、深度强化学习（Deep Reinforcement Learning）和策略梯度（Policy Gradient）等。
  - **总而言之，强化学习是一种通过试错和学习来获得最优行为策略的机器学习方法。它通过智能体与环境的交互和奖励信号来引导学习，以实现在给定环境下最大化累积奖励的目标**。

2. 马尔可夫决策过程（Markov Decision Process，MDP）
- **马尔可夫决策过程（Markov Decision Process，MDP）是一种用于建模序贯决策问题的数学框架**。它基于马尔可夫性质，**描述了在随机环境中，智能体通过采取不同的动作来达到目标的决策过程**。
- MDP由五个核心要素组成：
  1. 状态（State）：表示智能体所处的环境状态。**在每个时间步，智能体通过观察当前状态来做出决策**。
  2. 动作（Action）：表示智能体可以采取的行动或决策。**在给定状态下，智能体可以选择执行的动作集合**。
  3. 转移概率（Transition Probability）：**表示从一个状态到另一个状态的转移概率**。给定当前状态和采取的动作，转移概率描述了下一个状态的**可能性分布**。
  4. 奖励（Reward）：表示智能体在状态转移过程中获得的**即时反馈**。奖励可以是正向的（鼓励行为）或负向的（惩罚行为）。
  5. 折扣因子（Discount Factor）：**用于权衡当前奖励和未来奖励的重要性。折扣因子决定了未来奖励对智能体决策的相对重要性**。
- 基于这些要素，MDP的**目标是找到一个最优策略（Policy），使智能体在给定状态下能够选择最佳的动作以最大化累积奖励**。最优策略可以通过价值函数（Value Function）来衡量，价值函数表示从给定状态开始，智能体在执行最优策略下可以获得的长期回报。
- MDP的求解可以使用**动态规划方法，其中常用的算法包括值迭代（Value Iteration）和策略迭代（Policy Iteration）**。值迭代通过迭代更新价值函数来逐步逼近最优策略，而策略迭代则通过反复改进策略和更新价值函数来获得最优策略。**策略迭代算法在每次改进策略后都需重新进行策略评估，这通常比较耗时。策略改进与值函数的改进是一致的，因此可将策略改进视为值函数的改善**
- 马尔可夫决策过程在强化学习中具有重要的地位，被广泛应用于智能体在复杂环境中进行决策和规划的问题，如机器人控制、自动驾驶和资源管理等领域。

3. 总之，在环境中状态的转移、奖赏的返回是不受机器控制的，**机器只能通过选择要执行的动作来影响环境，也只能通过观察转移后的状态和返回的奖赏来感知环境**
4. 机器要做的是通过在环境中不断尝试而学得一个“策略”（policy）$\pi$。策略有两种表示办法：一种是将策略表示为函数$\pi : X \rightarrow A$，确定性策略常用这种表示；另一种是概率表示$\pi : X x A \rightarrow \mathbb{R}$，随机性策略常用这种表示。
5. **浅谈强化学习与监督学习的差别**
![20230614150202](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614150202.png)

## 16.1 K-摇臂赌博机（K-Armed Bandit），亦称“K-摇臂老虎机”
- 探索与利用
  - 强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：**最大化单步奖赏，即仅考虑一步操作。欲最大化单步奖赏需考虑两个方面**：
    - 每个动作带来的奖赏
    - 要执行奖赏最大的动作
  若每个动作对应的奖赏是一个确定值，那么尝试一遍所有的动作便能找出奖赏最大的动作。**然而，更一般的情形是，一个动作的奖赏值是来自于一个概率分布，仅通过一次尝试并不能确切地获得平均奖赏值**.
- **这样的单步强化学习任务对应了一个理论模型，即“K-摇臂赌博机”（K-armed bandit）**。K-摇臂赌博机有K个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。
- **若仅为获知每个摇臂的期望奖赏，则可采用仅探索”（exploration-only）**法：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。**若仅为执行奖赏最大的动作，则可采用“仅利用”（exploitation-only）**法：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。
> 显然，“仅探索”法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；“仅利用”法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。

- 事实上，**“探索”(即估计摇臂的优劣)和“利用”(即选择当前最优摇臂)这两者是矛盾的**，因为尝试次数(即总投币数)有限，加强了一方则会自然削弱另一方，**这就是强化学习所面临的“探索利用窘境”(Exploration-Exploitation dilemma)**。显然，**欲累积奖赏最大，则必须在探索与利用之间达成较好的折中.**

6. K-摇臂赌博机（K-Armed Bandit）是强化学习中的一个经典问题。**它模拟了在面临多个选择（摇臂）时，如何在有限的资源和不确定性环境下进行决策的情境**。
- K-摇臂赌博机的设定如下：
  1. 摇臂（Arms）：有K个摇臂可供选择，每个摇臂**都有一个固定的概率分布**，表示在选择该摇臂时获得奖励的概率分布。
  2. 奖励（Reward）：在每次选择摇臂后，根据所选择摇臂的概率分布随机产生一个奖励，奖励可以是实数值或离散值。
  3. 动作选择（Action Selection）：在每个时间步，智能体需要选择一个摇臂进行操作。选择哪个摇臂是智能体的决策。
  4. 目标：智能体的目标是通过与摇臂的交互累积获得最大的奖励。
- **在K-摇臂赌博机问题中，智能体需要在探索（Exploration）和利用（Exploitation）之间做出权衡。探索是指尝试选择未知的摇臂以了解它们的概率分布和奖励情况，以便获取更准确的信息。利用是指选择已知的高奖励摇臂，以最大化当前获得的奖励**。
- 解决K-摇臂赌博机问题的关键是找到一个合适的策略，即在探索和利用之间找到平衡点。常用的策略包括：
  1. ε-贪心策略（ε-Greedy）：以1-ε的概率选择当前奖励最高的摇臂，以ε的概率选择随机摇臂，以促进探索。
  2. 上限置信界（Upper Confidence Bound，UCB）：根据奖励的置信界，选择具有最高上界的摇臂，以平衡探索和利用。
  3. 梯度赌博算法（Gradient Bandit Algorithm）：使用梯度信息来调整选择每个摇臂的概率，以最大化奖励的期望。
- 这些策略都有不同的权衡，探索能力强的策略可能会获得更多信息，但可能在短期内获得较低的奖励；而利用能力强的策略可能会在短期内获得较高的奖励，但可能会错过更高奖励的摇臂。
- K-摇臂赌博机问题作为一个简单但具有挑战性的强化学习问题，可以帮助我们理解和研究在有限资源和不确定环境下的决策过程，并为更复杂的强化学习问题提供基础。

7. $\epsilon$-贪心法基于一个概率来对探索和利用进行折中：每次尝试时，以$\epsilon$的概率进行探索，即以均匀概率随机选取一个摇臂；以$1 - \epsilon$的概率进行利用，即选择当前平均奖赏最高的摇臂(若有多个，则随机选取一个).
> 若摇臂奖赏的不确定性较大，例如概率分布较宽时，则需更多的探索，此时需要较大的$\epsilon$值；若摇臂的不确定性较小，例如概率分布较集中时，则少量的尝试就能很好地近似真实奖赏，此时需要的$\epsilon$较小。通常令$\epsilon$取一个较小的常数，如0.1或0.01。**然而，若尝试次数非常大，那么在一段时间后，摇臂的奖赏都能很好地近似出来，不再需要探索，这种情形下可让$\epsilon$随着尝试次数的增加而逐渐减小**，例如令$\epsilon = 1/\sqrt{t}$.

8. Softmax
- Softmax算法基于**当前已知的摇臂平均奖赏**来对探索和利用进行折中。**若各摇臂的平均奖赏相当，则选取各摇臂的概率也相当；若某些摇臂的平均奖赏明显高于其他摇臂,则它们被选取的概率也明显更高.**
![20230614155654](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230614155654.png)
9. 对于离散状态空间、离散动作空间上的**多步强化学习任务，一种直接的办法是将每个状态上动作的选择看作一个K-摇臂赌博机问题，用强化学习任务的累积奖赏来代替K-摇臂赌博机算法中的奖赏函数**，即可将赌博机算法用于每个状态：对每个状态分别记录各动作的尝试次数、当前平均累积奖赏等信息，基于赌博机算法选择要尝试的动作。**然而这样的做法有很多局限，因为它没有考虑强化学习任务马尔可夫决策过程的结构**。若能有效考虑马尔可夫决策过程的特性，则可有更聪明的办法.
10. **考虑多步强化学习任务，暂且先假定任务对应的马尔可夫决策过程四元组E = (X，A，P，R)均为已知，这样的情形称为“模型已知”**，即机器已对环境进行了建模，能在机器内部模拟出与环境相同或近似的状况。**在已知模型的环境中学习称为“有模型学习”(model-based learning)**.
11. 在现实的强化学习任务中，环境的转移概率、奖赏函数往往很难得知，甚至很难知道环境中一共有多少状态。**若学习算法不依赖于环境建模，则称为“免模型学习”(model-free learning)，亦称“无模型学习”，这比有模型学习要困难得多**.
12. **在免模型情形下，策略迭代算法首先遇到的问题是策略无法评估，这是由于模型未知而导致无法做全概率展开**。此时，只能通过在环境中执行选择的动作，来观察转移的状态和得到的奖赏。受K摇臂赌博机的启发，**一种直接的策略评估替代方法是多次“采样”，然后求取平均累积奖赏来作为期望累积奖赏的近似，这称为蒙特卡罗强化学习**。由于采样必须为有限次数，因此该方法更适合于使用T步累积奖赏的强化学习任务.
13. **蒙特卡罗强化学习（Monte Carlo Reinforcement Learning）是一种基于样本采样的强化学习方法。它通过与环境的交互来获取经验数据，并利用这些数据进行策略评估和策略改进**。
- 蒙特卡罗强化学习的核心思想是基于回报样本的累积来评估策略的好坏。**具体而言，它通过执行一系列完整的回合（Episode）来采样轨迹数据**。在每个回合中，智能体与环境进行交互，根据当前的策略选择动作，并观察环境的反馈（奖励和下一个状态）。通过多次回合的交互，蒙特卡罗方法可以得到一组回合样本。
- 利用这些回合样本，蒙特卡罗强化学习主要包含两个步骤：**策略评估和策略改进**。
  1. 策略评估（Policy Evaluation）：在策略评估中，通过对回合样本的回报进行累积，估计每个状态的值函数或状态动作对值函数。一种常用的方法是使用蒙特卡罗预测（Monte Carlo Prediction），根据回合样本计算状态值函数或状态动作对值函数的平均回报。
  2. 策略改进（Policy Improvement）：在策略改进中，基于估计的值函数，可以对策略进行改进。常用的方法是使用ε-贪心策略（ε-Greedy Policy），以一定概率选择当前估计值最高的动作，以提高利用能力；同时，以一定概率选择随机动作，以增加探索能力。策略改进可以根据当前值函数的估计，生成新的策略。
- 通过反复进行策略评估和策略改进，蒙特卡罗强化学习逐渐优化策略，并逐步收敛到最优策略。
- **蒙特卡罗强化学习的优点是它不需要对环境的转移概率进行假设，只利用交互的样本数据进行学习。然而，由于需要采样大量的回合样本，蒙特卡罗方法通常具有较高的计算成本和样本效率。此外，它在处理连续状态和动作空间的问题上存在一定的挑战**。
- 总之，蒙特卡罗强化学习是一种基于样本采样的强化学习方法，通过采样回合样本来评估和改进策略。它是强化学习中重要的方法之一，为解决复杂的强化学习问题提供了一种有效的学习框架。

14. **时序差分学习（Temporal Difference Learning）是一种强化学习方法，它结合了动态规划和蒙特卡罗方法的优点。时序差分学习用于估计值函数或值函数的更新，通过观察到的即时奖励和预测的未来奖励之间的差异来进行学习**。
- 在时序差分学习中，智能体通过与环境的交互来获取经验数据。在每个时间步，智能体观察当前状态，选择动作并执行，然后观察环境的反馈（奖励）和下一个状态。智能体使用这些信息来**更新值函数的估计**。
- 时序差分学习使用**贝尔曼方程（Bellman equation）作为基本的更新规则**。贝尔曼方程描述了值函数的递归关系，将当前状态的值与下一个状态的值之间建立了联系。
- 具体而言，**时序差分学习中最常用的方法是Q-learning**。Q-learning使用一个Q值函数（动作值函数）来估计在给定状态下执行每个动作的价值。在每个时间步，Q-learning通过以下更新规则来更新Q值函数的估计：
$$Q(s, a) = Q(s, a) + \alpha [r + \gamma max_a' Q(s', a') - Q(s, a)]$$
> 其中，Q(s, a)表示在状态s下执行动作a的Q值，r表示即时奖励，s'表示下一个状态，a'表示下一个状态下的最优动作，$\alpha$表示学习率（用于控制更新幅度），$\gamma$表示折扣因子（用于平衡即时奖励和未来奖励的重要性）。
- Q-learning通过迭代地更新Q值函数，逐渐收敛到最优的Q值函数。一旦获得最优的Q值函数，智能体可以根据该函数选择具有最高Q值的动作作为策略。
- **时序差分学习的优点是它可以在每个时间步进行更新，不需要等待整个回合的结束，因此具有较高的样本效率。它还能够处理连续状态和动作空间，并且能够在未知环境中进行在线学习**。
- 总之，时序差分学习是一种强化学习方法，通过观察到的即时奖励和预测的未来奖励之间的差异来更新值函数的估计。Q-learning是时序差分学习的一种常用方法，通过迭代更新Q值函数来实现值函数的学习和策略的改进。

15. **值函数近似是一种在强化学习中用于估计值函数的方法。它的主要思想是通过函数逼近器（如线性函数、神经网络等）来近似值函数，而不是显式地存储和计算所有可能状态的值**。
- 在强化学习中，值函数用于估计每个状态或状态动作对的价值，表示在当前状态下采取行动的预期回报。然而，对于大规模状态空间或连续状态空间的问题，显式存储和计算值函数是不可行的。值函数近似通过使用函数逼近器来解决这个问题，将状态空间映射到一个参数化的函数空间中。
- 值函数近似的基本步骤如下：
  1. 选择一个函数逼近器：值函数近似使用函数逼近器来估计值函数。常见的函数逼近器包括线性函数、多项式函数、神经网络等。函数逼近器的选择取决于具体问题的性质和复杂度。
  2. 定义特征表示：为了进行值函数近似，需要将状态或状态动作对映射到函数逼近器所能接受的输入格式。通常，需要定义一组特征表示来描述状态或状态动作对的属性。这些特征可以是原始状态的函数变换、经验数据的统计特征等。
  3. 参数估计：使用强化学习算法，根据采集的样本数据，对函数逼近器的参数进行估计。常见的算法包括梯度下降、随机梯度下降、Q-learning等。通过与环境的交互，根据奖励信号和下一个状态的值函数估计，更新函数逼近器的参数。
  4. 策略改进：根据近似的值函数，可以使用贪心策略或其他策略选择方法来改进智能体的策略。根据值函数近似的结果，智能体可以选择具有最高估计值的动作作为策略。
- 值函数近似的优点是它能够处理大规模状态空间或连续状态空间的问题，节省了存储和计算的开销。然而，它也面临着一些挑战，如函数逼近器的选择、特征表示的设计和参数估计的稳定性等。
- **总结而言，值函数近似是一种通过使用函数逼近器来估计值函数的方法。它通过将状态映射到函数空间中，解决了大规模状态空间或连续状态空间问题中的存储和计算困难**。

16. **模仿学习（Imitation Learning）是一种强化学习的方法，旨在通过观察和模仿专家（或者已经解决任务的人）的行为来学习任务的策略**。模仿学习的目标是从专家的示范中学习到一个能够执行类似行为的策略，而无需进行探索和试错。
- 在模仿学习中，通常存在两种主要方法：**直接模仿学习和逆强化学习**。
  - 直接模仿学习（Direct Imitation Learning）：
    - 直接模仿学习是一种简单直接的模仿方法，它通过收集专家演示的样本数据，然后直接使用这些数据来训练一个模型，例如神经网络。模型会尽量复制专家示范的行为。在训练过程中，模型学习将输入状态映射到输出动作的映射关系，以实现与专家类似的策略。**直接模仿学习可以使用监督学习的方法来进行训练，其中输入是状态，输出是动作**。
    - **直接模仿学习的优点是简单直接，容易实现。然而，它也面临一些挑战。例如，直接模仿学习可能会受到专家演示中的噪声和偏差的影响，导致模型学到错误的策略。此外，直接模仿学习无法解决探索问题，因为它仅仅是复制专家的行为而不具备主动探索新策略的能力**。
  - 逆强化学习（Inverse Reinforcement Learning）：
    - **逆强化学习是一种基于观察者行为来推断其背后的奖励函数的方法**。在逆强化学习中，智能体试图从专家示范中学习到任务的奖励函数，然后根据这个奖励函数来执行任务。**逆强化学习的目标是理解专家示范的动机和目标，而不仅仅是模仿其行为**。
    - 逆强化学习的基本思想是通过比较专家示范的行为与候选奖励函数生成的行为来推断奖励函数。通过迭代的过程，逆强化学习算法可以找到与专家示范最一致的奖励函数。**一旦获得了奖励函数，智能体可以使用强化学习方法来学习最优策略**。
    - **逆强化学习的优点是它能够从专家示范中推断出更深层次的意图和目标，使得智能体能够更加灵活地适应环境和任务变化。然而，逆强化学习也存在一些挑战，例如推断奖励函数的不确定性、计算复杂度和过拟合等问题**。

总结而言，**模仿学习是一种通过观察和模仿专家的行为来学习任务策略的方法。直接模仿学习直接使用专家示范来训练模型，而逆强化学习通过推断奖励函数来学习专家的目标和动机**。这些方法在实际应用中具有各自的优缺点，可以根据具体问题选择合适的方法。